<html>
<head>
<title>SOAR</title>
</head>
<body>
<h1>SOAR - System of Automatic Runs</h1>
<h2>version 0.9.2</h2>

<UL>
<li>
<A HREF="#OverView">Overview</A>
<li>
<A HREF="#LikelySteps">Likely Steps to SOARing</A>
<li>
<A HREF="#Installation">Installation</A>
<li>
<A HREF="#TestProject">Testing Project</A>
<li>
<A HREF="#AdaptingProject">Adapting a Project</A>
<li>
<A HREF="#AdaptingSoar">Adapting SOAR</A>
<li>
<A HREF="#BuiltIn">Built In Projects</A>
<li>
<A HREF="#OptionFile">Options File</A>
<li>
<A HREF="#Report">Report</A>
<li>
<A HREF="#Throttles">Throttles</A>
<li>
<A HREF="#SquidUsage">HTTP Proxy Usage</A>
<li>
<A HREF="#UsingSoar">User Manual</A>
<UL>
<li>
<A HREF="#TimeActivated">Time Activated Tasks</A>
<li>
<A HREF="#AutomatedCode">Automated Code Replacement</A>
<li>
<A HREF="#ControlScript">Control Script</A>
<li>
<A HREF="#ControlScriptExample">Control Script Examples</A>
<li>
<A HREF="#ControlScriptOptions">Control Script Options</A>
<li>
<A HREF="#StatusScript">Status Script</A>
<li>
<A HREF="#StatusScriptExample">Control Script Examples</A>
<li>
<A HREF="#ScrubberUsage">Scrubber Usage</A>
<li>
<A HREF="#SoarRm">Soar_rm Usage</A>
<li>
<A HREF="#SoarToCondor">Soar_to_condor Usage</A>
<li>
<A HREF="#SoarVersion">Soar_Version Usage</A>
<li>
<A HREF="#FAQ">Frequently Asked Questions</A>
<li>
<A HREF="#WebInterface">Web Interface</A>
</UL>
<li>
<A HREF="#ReleaseNotes">Release Notes</A>
<li>
<A HREF="#WantedFeatures">Wanted Features</A>
</UL>

<A NAME="OverView"></A>
<h2>Overview</h2>
<!--
what it is
how it works in general
how it helps us solve problems
-->

<p>
SOAR is a framework which automates the execution of groups of jobs.
These jobs form a DAG (directed acyclic graph), are run under Condor,
and SOAR assembles the results within single directory.
<i>
For information on Condor visit: http://www.cs.wisc.edu/condor
</i>
</p>
<p>
The system is flexible, 
such that it can run all data sets at once,
or it can keep track of completions,
running new data sets on a periodic basis.
It reports on the progress by both logging information
and progressively plotting of the progress of the run
in terms of how many jobs are running and how many are ready to run.
The system becomes adapted and personalized for each project.
</p>
<h3>Theory</h3>
<p>
A single tie in so a program runs under SOAR does some things for you.
It maps a location for data sources to a program. It make an easy way
for you to either feed in new data sources or to create a new version
of the program to run against the existing data sources. The third
primary location(the tie to soar) is a set of scripts which fetch the data sources
to the location needed for each job within the DAG and scrapes the expected
results to the results location.
</p>
<h3>Operational Usage</h3>
<p>
SOAR is currently intended and written to be managed by one person
or possible several through a shared account. The product is young
but reliable and the focus has been on tools to easily automate
research. In time work will be done to explore making the various systems
of software play nicely in the area of multiple users and groups.
</p>
<p>
Most projects once adapted to SOAR sit in a mode where additional
jobs are automatically run as additional datasets are placed in that projects
data area. Multiple people can run the same research by simply having
a derived project thus establishing their own data and result locations
via their own portion of the web interface. SOAR uses Condor's ability
to run jobs on any sort of periodic basis needed. If the project needs
a sweep for new data every 4 hours, it is trivial.
</p>
<p>
SOAR is generally managed by a single person who watches over disk
consumption, adapts new research to SOAR projects, assist with
special runs and expediting software changes for projects that
rapidly change the science they are doing on the data.
</p>

<h3>Web Interface</h3>
<p>
No matter how one starts the runs, the web interface allows four things:
</p>
<ul>
<li>
Access to where the DAG is running to inspect various things.
<li>
A profile showing the jobs ready vs currently running over time
allowing one to estimate when all the results are available
and the run completed.
<li>
A report showing current status of every job updated periodically
just like the profile plot.
<li>
Access to where the results are stored so individual job's results
can be looked at as soon as each job completes.
</ul>
<h3>Hey, I know how to submit clusters with one submit file, why do I want to use SOAR</h3>
<p>
There are some distinct advantages to using SOAR. And it is very similar to
having a folder with data(each in a distinct loaction) and a single submit
file. The following are provided by SOAR only:
</p>
<ul>
<li>
Since SOAR uses DAGs and Condor_dagman, a SOAR run can be adjusted 
to have a minimal impact on your network and submit machine while
making runs easier. An example is that you can submit 1000s, but throttle
it back to a reasonable number and make the submit node's performance
be good for all.
<li>
There is a process which every 5 minutes produces a profile of the current 
set of jobs and a report of the status of each job. The profile is a plot
which shows number of jobs ready to run and the average number currently
running. This makes it a predictable task to know when all the results are ready
to be evaluated.
<li>
As jobs complete, each successful job has its data collected in a web accessible location
freeing the need to collect results by hand.
<li>
When all the runs are complete, a collection of all the results for a run
are either tared or zipped up into a single package for easy web access.
<li>
You'll never have to write another submit files again.
<li>
It's easy to start a new person using a particular application. One simply 
makes them their own project. You can even seed it with appropriate data sets.
From there they simply have to either modify the application and analysis
or feed the project a different set of data sets.
<li>
It changes the focus from "How do I make Condor do what I want" to
"What data do I want to run?", "How do I want to change the analysis?" 
and "How can I process the results the best?".
<li>
One gets to focus on the science!
</ul>


<A NAME="LikelySteps"></A>
<h2>Likely Steps towards SOARing</h2>
<!--
to organize the process for the beginner
-->

<p>
Each project will have its own unique steps leading to automation
and maintenance of the automated jobs.
Accounting for the unique nature of jobs,
each initial SOAR setup will follow these steps:  
</p>
<ol>
<li> download and unpack
<li> Follow the "<b>Installation</b>" steps below.
<li> Adapt the Project to SOAR
     <br>possibly modify the program to be automated, such that
it can be automated
<li> Adapt SOAR to the Project 
     (ready everything for the project):
   <br>copy and modify existing sample scripts and ???
   <br>stage input data sets and executables
   <br>set up time-based submission
<li> run condor_submit
</ol>
<p>
Steps 3, 4, and 5 will be repeated for each new project to be
automated.
</p>

<A NAME="Installation"></A>
<h2>Installation</h2>
<!--
step by step
-->
<p>
The first part to installing SOAR is deciding where to place things.
SOAR requires a minimum of 2 installation locations and one URL up to
four loactions and three URLS. This is a combination of limiting
what the web interface can access combined with allowing for
large amounts of disk to be consumed between initial datasets for
projects and copious resulting data from a system allowing
for an ease of doing research not easily reachable before.
See <b>Configurable locations</b> below. 
</p>
<p><b>
NOTE: SOAR is designed to leverage Condor_dagman within
a Condor environment. The run location must be on local disk
to avoid file locking issues with most shared file systems.
</b></p>

<ul>
<li>
Install Condor if you do not have it yet.
<li>
Get tar ball of SOAR distribution.
<li>
Configure a URL for the SOAR installation directory for Apache2.
See SOAR.conf for an example in the top level of the SOAR tar ball.
<li>
Optionally configure a RUN url if you plan on locating the condorruns,
tarcache and results folders other then in the installation directory.
<li>
Optionally configure a location for the results and a matching URL
<li>
Untar the tar ball and run ./soar_configure.pl --help to see current
install options.
<li>
Run the base install ./soar_configure.pl --install-loc=/some/path 
--soar-url=http://some path.
<li>
Edit per_plotsandreports per_runs so they have a good path and
a valid CONDOR_CONFIG settings. Path has to include Condor
binaries.

</ul>


<h3>
Software needed to run SOAR
</h3>
<ul>
<li>
Perl
<li>
PHP enabled for the web browser.
<li>
date/Manip.pm Perl module
<li>
Gnuplot 4.2 or newer ( for profiling )
<li>
Convert by ImageMagick ( for profiling )
</ul>

<!-- edited to this point in file -->

<A NAME="Test Project"></A>
<h2>Test project for installation testing</h2>
<p>There is a test project in the install
kit for testing and example purposes. Its called
"test_project". To test with it:</p>
<ul>
<li>
./control --project=test_project --version=v1 --kind=new --limit=1
</ul>


<A NAME="AdaptingProject"></A>
<h2>Adapting a Project to SOAR</h2>
<!--
requirements for the system, so the scientist/admin has a good
idea if their code CAN be adapted, and what will be required
of their code
-->
<p>
To effectively do multiple datasets in any system
you want your application to either 
accept command line arguments and read parameters from one or more files.
Anything hard coded into the program you are running can not be easily varied.

<h3>Configurable locations</h3>
<!--
why we have an fsconfig file, and what it does for us
syntax of the file
contents of the file
when and why we will modify this file in the future
-->
<p>
The SOAR system utilizes and organizes various components related
to a project's runs.
As these components may be kept in a variety of locations,
the file <tt>fsconfig</tt> identifies the component locations.
Each component listed is also the directory name.
Within the directory, each project may have its own subdirectory
for project-specific versions of the component.
</p>
<ul>
<li>
<tt>soar</tt>: Directory holds start up files for web interface
<li>
<tt>control</tt>: Where the master scripts are all located
<li>
<tt>source</tt>: Where project code and glue scripts live
<li>
<tt>results</tt>: Where results go on a per project basis
<li>
<tt>tarcache</tt>: Where large data sets are stored in compressed format for additional use.
<li>
<tt>condorruns</tt>: Space used to run the jobs and collect logs
<li>
<tt>imageruns</tt>: Places data sets can be found
</ul>

<p>
Additionally, each project can have its data location specified in this file in the format:

<p>
	Project_name,Location_holding _data_in _project_name_folder

<p>
Soar is told where to 
find datasets for your jobs. These will be folders with unique names with the 
variable data for your jobs or folders named datasetXXXXXXX which  will contain the 
unique job folders. The code and jobs folders under sources contain the unchanging 
parts for the first and the glue scripts in the second.
</p>

<A NAME="AdaptingSoar"></A>
<h2>Adapting SOAR to a Project</h2>
<h3>Source Directory code</h3>

<p>
This directory gets all the sources to make your job run. It also gets the results of 
compiling if that is something your job needs be it Matlab or some regular computer 
language. For security purposes it must have an .htaccess file or the code will not 
be placed where it needs to be for the job to find it. This is to ensure that the sources 
on the web are only accessible by authorized persons.
</p>

<p>
Normally all files in the code directory are copied to the submit location where the 
job is started. However any file listed in the file SKIP will not be moved.
</p>

<p>
Another file called BLACKLIST must exist. It contains a name which starts with a number, 
a colon the word blacklist and then a reason within [  ]. Here follows an example:
</p>
<pre>
1000: blacklist [ condensation ]
</pre>

<h3>
Source Directory jobs
</h3>

<p>
This directory holds the glue scripts which adapt SOAR to handling the data sets 
and the code of your research.  The glue scripts tie together the data sets to 
whatever processing you want to do.
</p>

<h3>
Glue Scripts and Interfacing files
</h3>

<p>
A basic job consists of a single node which submits to a pool and then another analysis job 
which could be run if desired based on results. A faulty start can have us execute a null 
piece of work for the first node and we usually do a null follow-up node. After all the 
jobs have run we have a report node,  an optional clean node, an after the report mode 
which preps the data collected if we are delivering it and a push the data node.
</p>

<p>
There are a number of scripts that run before or after which can or should be customized:
</p>
<ul>
<li>
<tt>Prejob.pl</tt>:  Get data files to run directories and can do some basic sanity checks
<li>
<tt>Postjob.pl</tt>:  Decide if analysis node will run and saves the desired results into the results location. It is the perfect place to trim out large files unique to your job from the run directory after you save them in results of course.
<li>
<tt>Postreport.pl</tt>:  save additional information into results and perhaps zip all the results up for easy web download
<li>
<tt>Pushdata.pl</tt>:  job submitted to pushdata which can be used to deliver the data remotely
</ul>

<p>
All the template files are filled in with variable data.
</p>

<ul>
<li>
<tt>Doextract.template</tt>:   this is a template of the submit file for the job and is close to what will be moved to the run location. This file tells Condor what the job to execute. This file gets moved automatically. The executable  is almost always a script which extracts the data, runs the real job and if needed packages the results. This file dictates what gets run, what gets moved and how to run the job. The final submit file is completed by Make_job_submits.pl.
<li>
<tt>Doextractnull.template</tt>:  when we discover issues in prejob.pl we substitute a dummy job so as to not waste a jobs which we know will fail. Job is classified as failing.
<li>
<tt>Holdnode.template</tt>
<li>
<tt>Postmovie.template</tt>:  This is a job which could do further work after the job runs. It is almost always replaced by do nothing job but provides a place for some extra work.
<li>
<tt>Postmovienull.template</tt>:  This is what we normally do after the job.
<li>
<tt>Postreport.template</tt>:  Runs the final report
</ul>

<p>
There are some additional files which allow extra features.
</p>

<ul>
<li>
<tt>Make_pushdata_submits.pl</tt>:  produce a customized node to push data somewhere
<li>
<tt>Make_job_dag_text.pl</tt>: though we normally run a single piece of work, we can insert between the prenode and postnode an arbitrarily complex dag for the actual work. Interesting note here is that this file writes a file called PROFILE. The name within will be used to find error, log and output files for profiling and plotting current status.
<li>
<tt>Make_job_submits.pl</tt>:  produce all job submits files from templates.
</ul>

<A NAME="BuiltIn"></A>
<h2>
Built In Projects
</h2>

We now have some standard projects 
which can be generated with SOAR. If you
point SOAR at a project input location
which includes a folder called "GENERIC"
and if it has the mandatory file "NEWPROJECT",
it will use the files there and the instructions
within "NEWPROJECT" to assemble the project.
This can be done for either R or Matlab
single phase projects. The PhaseOneBinary
is the script.R and for Matlab it is the 
name of the Matlab compiled executable.

<h3>
Tasks Done for Generic Project
</h3>
<ul>
<li>
Creates project access link on main SOAR web page
<li>
Creates a version "v1" of the project and places
it with proper <b>code</b> and <b>jobs</b> folders
in the source location for SOAR.
<li>
Places the project in continuous.cron so that
it will start running jobs when the data sets
can be seen.
<li>
Maps the location where the input will be to the project
in <b>fsconfig</b>.
</ul>

<h3>
Built In Project Config File(NEWPROJECT)
</h3>

The follow defines are mandatory:<br><br>

AccountingGroup = CHTC<br>
Department = CHTC - bt<br>
Project = replicateR<br>
InputLocation = /home2/bt/rundata<br>
PhaseOneName = process<br>
PhaseOneBinary = soartest.R<br>
PhaseOneType = R<br>
PhaseOneResults = meanx.csv<br>
Version = {R gets sl5-R-2.10.1, Matlab gets R2009bsp1}<br><br>

These defines are optional:<br><br>

PhaseOneLibrary = qtl_1.16-6.tar.gz,qtlnet_0.9.4.tar.gz<br>
PhaseOneExtraFiles =<br>
Options = osg<br><br>
Memory = 3
Priority = 10
Requirements = xxxxxxxx

Uses:<br><br>

AccountingGroup: Condor Accounting Group for Job<br>
Department: Description for owner of project on web interface<br>
Project: New Project Name<br>
InputLocation: data location for project<br>
PhaseOneName: Name used for logs<br>
PhaseOneBinary: Program being called<br>
PhaseOneType: Type of Built In project<br>
PhaseOneResults: Comma separated list of patterns to collect as results<br>
Version: Maps runtime to be used<br><br>

PhaseOneLibrary: Comma separated list of libraries in install order<br>
PhaseOneExtraFiles: Other files needed on the execute node<br>
Memory: Units are gigabytes of ram requested
Priority: Lets the job priorities be lower or higher then normal
Requirements: Completely replaces default requirements
<h3>
Built In Projects Data Layout
</h3>

Input data for jobs must be within a folder which is 
itself within another folder which begins with the name 
"dataset". So jobs which takes a single file "param.txt"
would look like this:<br><br>

projectname/dataset_test/job1/param.txt<br>
projectname/dataset_test/job2/param.txt<br>
projectname/dataset_test/job3/param.txt<br><br>

If there are shared files of data which each job uses
like "params.dat" they would be in a "special" folder 
called "shared":<br><br>

projectname/dataset_test/shared/param.dat<br><br>

Naming of the job directories and dataset_XXXXXXXXXX folders
must follow only a fee basic runs:
<ul>
<li>
Job directories must not be named "shared".
<li>
Name must not contain spaces(" ").
<li>
Name must not contain hyphens("-").
<li>
Name must not contain periods(".").
</ul>

<h3>
Built In Projects Types
</h3>

We have, as of release 0.8.3, the ability to generate a single phase
R project, a single phase Matlab project or a single phase binary
project. These are projects which take input files and run a single 
piece of code against them to produce results.
All projects must follow the data layout specified above.

<h3>
Sample Built In Project Definition Files
</h3>
<h4>
Matlab:<br>
AccountingGroup = CHTC<br>
Department = CHTC - bt<br>
Project = FirstMLauto<br>
InputLocation = /home2/bt/rundata<br>
PhaseOneName = process<br>
PhaseOneBinary = smalljob<br>
PhaseOneType = Matlab<br>
PhaseOneExtraFiles =<br>
PhaseOneResults = sample_file_plus5<br>
Options = (CHTC: as we ready R and Matlab into OSG, osg is also an option)<br>
Version = (see above)
</h4>
<h4>
R:<br>
AccountingGroup = CHTC<br>
Department = CHTC - bt<br>
Project = FirstRauto<br>
InputLocation = /home2/bt/rundata<br>
PhaseOneName = process<br>
PhaseOneBinary = soartest.R<br>
PhaseOneLibrary = qtl_1.16-6.tar.gz,qtlnet_0.9.4.tar.gz<br>
PhaseOneType = R<br>
PhaseOneExtraFiles =<br>
PhaseOneResults = meanx.csv<br>
Version = (see above)
</h4>
<h4>
Binary:<br>
AccountingGroup = Ece_Hagness<br>
Department = ECE - Hagness<br>
Project = SiExch<br>
InputLocation = /home/kjwillis/rundata<br>
PhaseOneName = main<br>
PhaseOneBinary = main<br>
PhaseOneType = Binary<br>
PhaseOneExtraFiles =<br>
PhaseOneResults = dat<br>
</h4>
<h3>
Configuration changes available for all GENERIC projects
</h3>
<li>
<b>+condorjobadd</b> - Any line starting in "+" is assumed to
be a Condor Job Ad and will be passed in to every job. Something
like "+Group = Spalding" is what is expected.
<li>
<b>Priority</b> - allows one to rasie or lower this set of 
jobs every time in relation to work in the queue already or following.
<li>
<b>Memory</b> - It is assumed that jobs run up to 2 Gigabytes of
memory usage. It is very important to ask for the memory limits needed
or many of your jobs could end up on the same execute node
and take yours and other's jobs into a very poor execute environment on
a thrashing machine. Usage is like "Memory = 4" which will
rewrite the requirements for the job to have a request for 4 Gigabytes
of usable memory per job.
<li>
<b>Requirements</b> - Builtin requirements currently call out
a particular R or Matlab environment with a possible memory
change. Using this configuration changes the requirements for all
these jobs is to this string.

<A NAME="OptionFile"></A>
<h2>
OPTIONS File
</h2>
<ul>
<li>
build = condorc                 # compiles of R packages and Matlab sent to another system
<li>
debug = true                    # leans more logs behind after a set of jobs completes
<li>
R_grid = condor e089.chtc.wisc.edu cm.chtc.wisc.edu          # R package build system
<li>
matlab_grid = condor submit.chtc.wisc.edu cm.chtc.wisc.edu   # Matlab compile system
<li>
skip_file_checks = true         # Sets _CONDOR_SUBMIT_SKIP_FILECHECKS in environment
before submissions of dag.
</ul>


<A NAME="Report"></A>
<h2>
How the report gets generated
</h2>

Either your job or postjob.pl glue script generates a file called RESULT which holds a 
number. When the report  process runs, it looks this number up in the file RESULTVALUES in 
your job directory which holds the glue scripts. This file has three fields separated 
by /. Field one is a number. Field two is either passed or failed. The last filed is 
the message which will be used to classify that result.

<A NAME="Throttles"></A>
<h2>
Scheduler Universe Throttles and SOAR
</h2>

In general, one wants to offload as much work as 
possible to remote machines and to not overly burden
the submmit host. Sometimes a process runs so fast
it makes sense to add it to the workflow as a Condor
Scheduler Universe job and have it run on the submit
node controlled by condor_dagman. Since SOAR creates
a DAG splice for each job, throttles in the splices
created by make_job_dag_text.pl are unlikely to
throttle anything but parts of that one job. However
we now have Global categories which allows for the
main DAG created by control to throttle a DAG created
mostly from splices. Here is how to add throttles
within SOAR for any node of the DAG.

<ul>
<li>
Add a Global category for the node in make_job_dag_text.pl
like this using a "+" before the name.<br>
print SUBM "JOB $job select.cmd DIR $unique\n";<br>
print SUBM "CATEGORY $job +SELECT\n";<br>
<li>
Create a file called "THROTTLES" with the desired limit
for each Global Category defined in this way which looks
like this:<br>
+BEAM 30<br>
+INIT 30<br>
+SELECT 30<br>
<li>
On the next run of control/SOAR, throttles will be made in the parent 
DAG for each Global category done in this way.
</ul>

<A NAME="SquidUsage"></A>
<h2>
Http Proxy Usage
</h2>
<p>
SQUID use: The goal is to be able to configure some projects to have their
data place where it can be dispersed by URL off some other server. So we 
have a file in the control location called SQUID which can have a project
and its squid location entered:  "projectA,squidlocA".
</p>
<p>
If we detect that, we take the jobs which will be run this time and copy
that input data to that location. Matching this we have to have the cleaning
node at the end of the run look for the jobs in that dag and remove the 
matching data from the squid location.
</p>
<p>
This only controls the data getting to the squid and being removed.
We will pass a squid location along with the current data location
to both make_job_dag_txt.pl and make_job_submits.pl. The last one 
will simply have to drop a URL for the data instead of the normal
path to the folder with it.
</p>
<A NAME="UsingSoar"></A>
<h1>
Using SOAR
</h1>

<A NAME="TimeActivated"></A>
<h2>
Time activated tasks
</h2>

<p>
The most convenient way to do production with SOAR is to place entries in a file which 
condor manages the frequency of.  There are two included(per_runs and per_plotsandreports). 
Per_runs fires off the commands in continuous.cron once a day and 
per_plotsandreports fires off the commands in checkprogress.com every 5 minutes. 
Setting this up is as easy as placing what you want done similar to the sample files 
and submitting them with condor_submit(condor_submit per_runs). The first usually 
contains usage of the control script and --kind=new so tracking of datasets already done 
has only new datasets run. The second as of version 0.7.5 is done for you. Every run
gets an entry added when the run is started and is removed when the run completes.
This allows us to use the information in the report system to accurately move jobs which were
running to complete. This information allows the next run to search out all currently running jobs and not start them again.
</p>
<p>
<b>If you need to remove a run, you must use the soar_rm script to
both extract this set of reports from the recheck interval or you waste
the cycles on reports on a completed/removed run and removes it from the RUNNING list
allowing it to be included in the next run of jobs</b>.
</p>
<p>
This way once you tie a project to do a particular job, you or the person doing the 
research only need to worry about creating more data sets into the image location for 
the project and pull results from the result location.
</p>
<p>
A third one was added with version 0.8.5. Per_scrubber runs a disk cleaning program
if submitted to Condor. This scrubber can watch a list of file systems listed
in the control directory. Look under documentation on "scrubber".
</p>

<A NAME="AutomatedCode"></A>
<h2>
Automated Code Replacement
</h2>
<p>
Input data for jobs in folders or folders in datasets are located either
in the directory specified by IMAGERUNS is control/fsconfig, or in a location
specified by the project name in the same location. Lets say your data is expected to 
be in your home in a directory rundata. So job data for project redapple
would be in /home/me/rundata/redapple.
</p>
<p>
The way code replacement works is that anything placed in /home/me/rundata/redapple_objs
is compared against the age of the current files for the currently requested version
of your workflow code. Newer code from that location is inserted into your work flow.
You have some control though its minimal at this point. The following attributes 
can be set in /home/me/rundata/redapple_objs/objconfig. The contents of this file will
only be active if files are found to update.
</p>
<p>
The version to have the binaries updated will be the version which is currently
running. This means if you are currently cross running 2 versions against
the same data, that autoupdating one version will end up updating the second 
version the next time it runs too.
</p>

</p>
<ul>
<li>
kind = reset 
<p>
(maybe you usually run with --kind=new but you want to run all your data
with the new code. This will run every data set you have.)
</p>
<li>
limit = 2
<p>
You want to set up for rerunning all of your data but want to inspect results from just 2 runs
first.
</p>
<li>
<li>
whitelist = "comma separated list of jobs"
<p>
If, when doing a code update, one wants to run a few jobs they can
be separated by commas. If the jobs are not in a dataset then they would
be listed like this:<br>
whitelist = job1,job2,job3,job4<br><br>
If the jobs are in a dataset, it would be like this:<br>
whitelist = dataset_XXXXXXX-job1,dataset_XXXXXXX-job2
</p>
<li>
dataset = dataset_XXXXXXX
<p>
This will fire off one job for each job in the dataset.
<b>(USING WHITELIST AND DATASET AT THE SAME TIME WILL
BRING UNEXPECTED RESULTS. USE ONE OR THE OTHER)</B>
</p>
<li>
matlabtarg = main.m,another.m
<p>
In this case all the scripts used by these two M files
will be Matlab compiled before your jobs are run. 
</p>
<li>
rversion = sl5-R-2.10.1<br>
rlibrary = qtl_1.16-6.tar.gz,qtlnet_0.9.4.tar.gz
<p>
R library updates must be built for the runtime
they plan to run with. These two lines provide
all the information needed to get these packages 
built and replaced within the project code location.
Note: packages are built from start to last on list.
</p>
</ul>
<p>
So we don't build software but the update works because we replace a binary
with a binary or an R script with a newer one or we even replace Matlab 
scripts/programs.
</p>
<p>
At this point this is the only
processing we do. Soar is not a build system but matlab jobs can not be run
unless we compile them. R packages can not be updated unless they are built
against the runtime being used.
</p>

<A NAME="ControlScript"></A>
<h2>
 Control Scripts
</h2>

Control

<ul>
<li>
	-b/--bail - Debug hook to stop just before submitting the dag which was built for the run
<li>
	--cache -  removes the C option and leaves the entire tar cache intact
<li>
	--clean=s - A(ALL), D(DATA), C(CACHE), R(RESULTS), L(LOGS and misc in rundir) Be very careful with this as its intent is to save space after your results have been saved. The only safe options are C and L which are also the defaults.
<li>
	--code=s - used when one wants to change the contents of the code directory for a new version which uses the glue scripts from a previous version. 
<li>
	--debug -  removes the L option and leaves the entire run directory intact
<li>
    --deepdata - job data at uncertain depth and not dataset format
<p>
--deepdata=none means to exclude no lowere or upper directories where
--deepdata=dir1,dir2,dir3 will skip looking in these directories
no matter where they are found.
</p>
<li>
	-d/--datasets=s - run all the jobs in a particular dataset folder
<li>
	-f/--first - Create the ENVVARS project file for a new project
<li>
    --generic=location - Look for GENERIC project creation folder here
<li>
	--glue - preserve old way of glueing jobs into master dag for
	unconverted projects.
<li>
	-h/--help - See this
<li>
	-k/--kind=s - install, reset, new, DATASET else a label for web
	<p>DATASET: A file is located in the input location for the project called
	DATASET. The dataset named in this file will be entered as if "--datasets=name"
	had been entered and --kind will be set to "new".</p>
<li>
	-l/--limit=N - start this number of jobs(acts like run)
<li>
	--maxidle=N - Limit submited jobs (default 500)
<li>
	--maxpre=N - Throttle the prescripts to minimize submit node IO(default 2)
<li>
	--maxsubmits=N - By default Condor_dagman submits 5 nodes per scan interval which
	defaults to 5 seconds. We don't have a knob for the scan interval but you can rev
	up how many it will process each time through a cycle.
<li>
	--norun - allows one to build the dag and submit files during development
	and not actually submit the dag and thus inspect how the glue scripts
	are functioning.
<li>
	--pattern - Used with --kind=reset and a project and version. This allows a partial
	reset.
<li>
	--project=s - Run which project?
<li>
	--preproject=s - Based on this earlier project, derive a new one
<li>
	--preversion=s - Based on this earlier version, derive a new one
<li>
	-s/--softversion - print software version
<li>
	--type=s - One type of install is a new param.dat from the imagedir for the project to it's code location
<li>
	--throttle=s - Add throttling to particular portion of dag.
<li>
	-u/--update - Update the project ENVVARS file to the last run.
<li>
	-v/--version=s - Run which version(name this version for install)?
<li>
	-w/--white=s - File to use for a whitelist based project run
</ul>

<A NAME="ControlScriptExample"></A>
<h2>
Control Examples
</h2>

<ul>
<li>
<pre>
./control --kind=refreshindex
</pre>
In the HTML location, where this manual is installed, there is
a file called "project.webinput" which is modified by some of
the SOAR functions. This can be modified by hand. This command
will recreate the SOAR home page with those contents.
<li>
<pre>
./control --generic=/tmp
</pre>
An input folder called GENERIC needs to be processed
into a new project. This control folder is in /tmp.
<li>
<pre>
./control --project=gravitropism --version=v3 --limit=10 --kind=reset --clean=CL
</pre>
The above will run an arbitrary 10 jobs from the available data sets for project
gravitropism version v3 and after will clean the run directory some and clear
any use of the tarcache space. Results and data sets are not touched.
<li>
<pre>
./control --project=gravitropism --version=v3 --kind=white --white=/tmp/whitelist
</pre>
The above will run a predefined set of your data against a particular
version of a particular project.
<li>
<pre>
./control --project=gravitropism --preversion=v3 --version=v4 --kind=install   --code=/tmp/code.tar.gz
</pre>
<li>
<pre>
./control --project=gravitropism --version=v3 --kind=reset # switches ENVVARS file
</pre>
The above will move history for the specified version to be reset allowing
all the jobs in the input job location to be run with the next --kind=new
<li>
<pre>
./control --project=gravitropism --version=v3 --kind=reset --pattern=xxxxxxxx
</pre>
The above will move history for the specified version to be reset allowing
all the jobs in the input job location which match the pattern to be run with the 
next --kind=new. The match to the pattern expects the match to happen at
the beginning of the job name. It would match one or more datasets
or one or more jobs within any existing scheme of chosen names as long
as the job <b>starts with the pattern</b>.
<li>
<pre>
./control --project=gravitropism --version=v3 --kind=new
</pre>
This is the most basic use. Run only data sets of this project and version
which have not been run yet. If run once a day, new data can be prepared
and placed in the projects data location and the new data sets will all
be run.
<li>
<pre>
./control --project=gravitropism --version=v3 --kind=white --white=/tmp/whitelist
</pre>
<li>
<pre>
./control --preproject=natehighres --preversion=v1 --project=DanLewisArabidopsis --version=v1 --kind=install
</pre>
<li>
The above will create a new project based on an existing projects and place the version
at any level desired. This is how one might create tracking and data set freedom for
a number of people running the same job but with different data and parameters.
<pre>
./control --project=gravitropism --version=v3 --kind=install --type=param.dat
</pre>
</ul>

<p>
Each project has a master file ENVVARS in the run directory,
which is used  various things.
It is changed out only for --kind=new.
However, if it comes out of sync or to make it the last "reset",
the command is 
<pre>
./control --project=gravitropism --update
</pre>
with one of the following additional command line options:
<ul>
<li>
<pre>
--kind=install
</pre>
to set up a new version of code and compile
<li>
<pre>
--kind=reset
</pre>
Erase tracking of completed jobs and those currently running
to allow all to run again.
<li>
<pre>
--kind=new
</pre>
to do all the new data sets since the ones run by <tt>--kind=reset</tt> 
<li>
<pre>
--kind='other text'
</pre>
to do a full run,
unless limited with 'Other Text' labeling the web page for the run
</ul>

<A NAME="ControlScriptOptions"></A>
<h2>
Control Options File
</h2>
<p>
One can establish an OPTIONS file for your site which will
override command line settings or override defaults. The file 
"OPTIONS" sits in the control directory and values parsed
there are applied back into the options hash. This was added
for channeling compiles of Matlab and R packages to another
system with Condor-C. Below is an example:<br>
<br>
build = condorc<br>
debug = false<br>
grid_resource = condor submit.chtc.wisc.edu cm.chtc.wisc.edu<br>
skip_file_checks = true
R_grid = condor e089.chtc.wisc.edu cm.chtc.wisc.edu
matlab_grid = condor submit.chtc.wisc.edu cm.chtc.wisc.edu
<br>
</p>
<A NAME="Status"></A>
<h2>
Status
</h2>
<p>
The framework usually runs this script at end of a job run.
</p>
<p>
Its command line options are
<ul>
<li>
<tt>-d</tt> or <tt>--display</tt>
<br>display report file in REPORT....
<li>
<tt>-e</tt> or <tt>--env</tt>
<br>run this environment file to find run
<li>
<tt>-h</tt> or <tt>--help</tt>
<br>summarize the options and quit
<li>
<tt>-k</tt> or <tt>--kind</tt>
<br>summary(report), profile(plot), whitelist, imagesz
<li>
<tt>-l</tt> or <tt>--last</tt>
<br>find most recent environment file to find run
<li>
<tt>-m</tt> or <tt>--match</tt>
<br>use this pattern to extract the whitelist
<li>
<tt>-p</tt> or <tt>--period</tt>
<br>sample interval
</ul>

            
<A NAME="StatusExample"></A>
<h2>
Status Examples
</h2>

<ul>
<li>
<pre>
./status --project=gravitropism --kind=profile --last
</pre>
Create the profile plot for the last or currently running set
of jobs for this project.
<li>
<pre>
./status --project=gravitropism --kind=profile --env=573
</pre>
Create a profile plot for a particular run.
<li>
<pre>
./status --project=gravitropism  --kind=summary --last
</pre>
Create the report for the last or currently running set
of jobs for this project.
<li>
<pre>
./status --kind=summary --project=gravitropism --env=573
</pre>
Create a report for a particular run.
<pre>
./status --kind=whitelist --project=med_mc2 --env=573 --match="fail \[No result files\]"
</pre>
Create a white list from the section of the report labeled "fail [No result files]"
</ul>

<A NAME="ScrubberUsage"></A>
<h2>
Scrubber Usage
</h2>

<p>
This program is set up to be activated on a daily basis by going
to where all the .cron files are and going: "condor_submit per_scrubber".
This will remove runs older then 60 days. This leaves results
alone.
</p>

<p>
Both of the following do the same thing due to defaults.
</p>

<ul>
<li>
./scrubber
<li>
./scrubber --mode=runs --grace=60
</ul>

<p>
To remove runs and results which are 45 days old you would do this:
</p>

<ul>
<li>
./scrubber --mode=all --grace=45
</ul>


<A NAME="SoarRm"></A>
<h2>
soar_rm Usage
</h2>

<p>
When a run is started you are givin an environment strings which describes 
where the data for that particular run is. "soar_rm" uses this to both
find the actual job id to use with "condor_rm", but also uses it to remove 
the run specific periodic "status" runs which update the plots and reports
at some interval. It also removes the jobs that were removed from those
which are believed to be running allowing the next run batch to rerun these jobs.
</p>

<ul>
<li>
./soar_rm --project=gravitropism --env=573
</ul>

<A NAME="SoarToCondor"></A>
<h2>
soar_to_condor Usage
</h2>

<p>
Want to know the condor job for the Condor_dagman job running your
SOAR run? Ask.
</p>

<ul>
<li>
./soar_to_condor --project=gravitropism --env=573
</ul>

<A NAME="SoarToJobINfo"></A>
<h2>
soar_to_jobinfo Usage
</h2>

<p>
Want to know where all the job information lies for your
SOAR run? Ask.
</p>

<ul>
<li>
./soar_to_jobinfo --project=gravitropism --env=573
</ul>

<A NAME="SoarVersion"></A>
<h2>
soar_version Usage
</h2>

<p>
Want to know the  version of SOAR?
</p>

<ul>
<li>
./soar_version
</ul>

<A NAME="FAQ"></A>
<h2>How do I ......</h2>
<p>
You probably have a project running in SOAR but you want one of the following changes:
</p>
<h3>Get someone else using the configured project</h3>
<p>
Base a new project on the current project.
</p>
<p>
Let say you have a project <b>positioning</b> and the best version of it is
<b>v3</b>. Your new user is <b>sam</b> and you want it to be called <b>
sams_liquids</b>.
</p>
<ul>
<li>
Add a line in "control/fsconfig" to have a location for this persons
data sets for their project. It would look like this: 
<b>sams_liquids,/home/sam/imageruns</b>.
<li>
Enter the following command: <b>./control --project=positioning --preversion=v3 
--newproject=sams_liquisams_liquid --version=v1 --kind=install</b>
</ul>
<h3>Change how the application works(change the science)</h3>
<p>
Generate a new version of the code.
</p>
<p>
This is very easy to do. If you make a new version you can name it 
something meaningful to mimic why you created it. The new version
now has access to all the project data allowing you to change the science
you are doing. If the project is <b>positioning</b> and the old version
is <b>v1</b> and you want the new version to be <b>surfaces</b> then
go to "sources/positioning" and enter this command: <b>cp -r v1 surfaces</b>.
</p>
<p>
Now simply place new binaries in "sources/positioning/surfaces/code".
</p>
<p>
<i>NOTE if you change the behavior and files needed or created you'll likely need
need to change the scripts "prejob.pl, postjob.pl and pushdata.pl".</i>
</p>


<A NAME="WebInterface"></A>
<h2>Web Interface</h2>
<!--
what can be seen
how to see it
-->
<h3>Basics</h3>
<p>
The goal of the web interface is to inspect a run of data sets done
as a single Condor DAG. One gets access to the run directory for the 
DAG which has a subdirectory for each job, to the plot showing current progress
of that DAG, a report which breaks out aspects of each job which has currently
ended, and allows access to results for each job in the DAG when it completes.
</p>
<h3>index.html</h3>
<p>
This file sits at the top most level and has sample links to projects.php.
These two files make it possible to have links created
as described above.
<p>
As of version 0.8.1, we generate it from the file "projects.webinput"
in the control directory. Format of file looks like:
<br>
Biochemistry - Attie,atti_yeast<br>
Limnology - GLEON,lake_study<br>
ECE - Van Veen,eeg_sim<br><br>
Note no space after the comma.
<br>
<A NAME="ReleaseNotes"></A>
<h2>Release Notes</h2>
<h3>Fixes since release</h3>
<h3>Version 0.9.2</h3>
<ul>
</ul>
<h3>Version 0.9.1</h3>
<ul>
<li>
New OPTION file entry for using Matlab and R
vanilla build slots. 
<li>
chtcutils work. soar_resources include the RPM making
tools for chtcutils.rpm production which has basic
Matlab abd R build slot files and a wrapper script
for Matlab compiles(chtc_mcc) and another for
R Library builds(chtc_buildRlibs). Both of these scripts
simply hide the complexity of building R libraries
and getting Matlab compiles to work. One passes
in the correct arguments, a condor job is created
and submited followed by waiting for the results
to come back.
<li>
switchover from first generation chct wrappers
rungeneric.pl and runR_Matlab.pl to chtcjobwrapper
and chtcinnerwrapper.
<li>
chtcinnerwrapper no longer supports old parameter
position behaviour and no longer accepts "--new".
</ul>
<h3>Version 0.9.0</h3>
<ul>
<li>
Create soar_to_jobinfo tool to get all
the information relative to a SOAR run.
<li>
Insert SoarID JobAd into top level
condor_dagman so given a condor cluster
one can get the SoarID to use the tool.
</ul>
<h3>Version 0.8.9</h3>
<ul>
<li>
We not capture version related information
in a http accesible file telling us urls
for runtime and its checksum, tarball name
and the root of the tree for this version
of the runtime.
<li>
We are using a files called "SQUID" to allow
the glue for a project to change the kind
of submit file it writes. An entry similar to
this would say that project eeg_simJC has 
a send to squid location of /home2/squid/SOAR.
<p>eeg_simJC,/home2/squid/SOAR</p>
<li>
We now can relatively place job data where
it can be served up with a proxy server and
remove the data when the run is done. Space
requirements are checked before data 
placements.
</ul>
<h3>Version 0.8.8</h3>
<ul>
<li>
We have upgraded our ability to run Freesurfer
software with a new double wrapper allowing us
to be sure of using the master wrapper in our HTTP
squid location.
<li>
Worked on automatic Matlab compiles and R package builds
which were not as I wanted them. We now only build once 
per code change and do on next swing around actually
check that the compile worked.
<li>
Deepdata location of arbitrary job directories
with no particular depth specified. We are converting 
the normal operation of non dataset directory
structure to adjust to the directory level it needs
to find the jobs. A job is any directory with only
data files and no directories. But one can specify
a comma separated list of directories to skip in case there
are special files which are not part of a SOAR job.

Whitelist handling of these also will work as long
as new subroutine in Soar.pm is used to translate
dash separated list of folders to slash separated path to job.
<li>
White list generator in status would spit out blank
line when splitting on a space and there were several
in a row. Empty lines are now skipped.
</ul>

<h3>Version 0.8.7</h3>
<ul>
<li>
Job io is never placed into the run location
where the DAG is created causing extra IO
via tar and gzip plus a movinging of the data.
Instead when the submit file is written. That
job data directory itself is moved as IO by
condor. In a similar fashion, "shared" is now 
a special dierectory and if present, that directory
will be directly moved by Condor's file transfer
mechanism. This removes the uncertainty of having the
correct shared data when multiple datasets are seen at once 
and makes the first node's prescript devoid 
of any real work, and thus is now gone.
<li>
Fixed scrubber install to connect to right script.
<li>
Fixed save of install time args for future update
help.
<li>
Shifted around code allowing for single arg control
script usage for refreshing SOAR index page for
users and for starting to process a GENERIC starter
project.{--generic=location or --kind=refreshindex}
<li>
We have added another automatic action in runR_Matlab.pl
which is our master R and Matlab wrapper. If there is a tar
ball named "SLIBS.tar.gz", it is expected to hold a directory
"SS" holding additional shared libraries. It will be extracted
and added to LD_LIBRARY_PATH for any project using the
wrapper.
<li>
Major IO change. SOAR now places job input folder
into the transfer list avaoiding need for prescripts 
to do more then ensure shared files get placed correctly.
<li>
Installer now remembers what options were used to
install last time in the control directory in a file called
"LASTINSTALL".
<li>
Tar Balls for runtimes fetched from Master CHTC
http server and are not included in release.
<li> 
OPTION file support: See above. Basically some things can
be established per SOAR like where R package builds go
and where Matlab compiles go. Also allows option
to have Condor not check files permissions on submit.
<li>
Local Universe compiles of Matlab and R packages not
supported due to CHTC running jobs around campus and
out in Open Science Grid which takes a particular
environment to do this.
</ul>
<h3>Version 0.8.6</h3>
<ul>
<li>
We now have choices to do Matlab compiles or R package builds 
on the fly with either LOcal Universe(on the same submit node)
or Condor-C(Another submit node) with recovery should it fail.
<li>
We have standardized our running of R and Matlab jobs
to use the same wrapper script and to fetch the runtime 
from a web server. The basic wrapper fetches the current 
master wrapper allowing the master wrapper to be changed
during large runs and relocate the URL for the runtimes.
<li>
OSG is an option for a new project on submit systems
that are configured to use the Glidein Factory and therefore
flock to the Open Science Grid.
<li>
</ul>
<h3>Version 0.8.5</h3>
<ul>
<li>
There was a bug in how the automated code replacement
worked for compiled matlab jobs such that we were always
running a version behind. This was for local universe
and condor-c compile and verify methods.
<li>
scrubber and status programs correctly handle end of year 
situations.
<li>
Matlab GENERIC projects can be wraped with thrid party
checkpointing code. This is still in verification mode
and not a Condor approved solution yet. Product being used
is DMTCP.
<li>
R binaries and the "Rscript" are removed. R on execute nodes
are no longer required. We take a built but not installed R
and install where we need to do what we need.
<li>
OPTION file created to allow defining grid_resource
needed for Condor-C job submission.
<li>
Condor-C build method to some central system substantiated.
Tested from gamgee.cs.wisc.edu and works once proper
change is made to a submit nodes client authentication methods.
<li>
Scrubber given full ownership as an automatic job once a day
to keep SOAR production space clean. Options documented above
with other programs.
</ul>
</p>
<h3>Version 0.8.4</h3>
<ul>
<li>
Add a scrubber for runs and results.
<li>
Still testing R and Matlab compile recovery
with new option "--build=localu" which creates
a condor local universe job to do the work
and before the expected code is used for a
run, a recheck is needed to ensure code compiled
before next jobs are run.
<li>
--build framework opens door to using Condor-C
jobs to have a central system prepare entire
pool worthy results for builds.
<li>
Fixed R libraries to be built before being
sent out on a job.
<li>
Fixed generic R and Matlab jobs to remove
any shared files extracted for a job to prevent
condor from returning them after the job. Need
to check binary generic jobs for same issue.
<li>
Standard configuration parameters were added to the
specification file for new generic projects. See
Memory, Priority, Requirements and +Jobad.

<li>
Ensure that a project code copy of wrapup.pl
or cleanup.pl trumps the generic one always used to
remove run from preiodic reports and plots and cleans up.
</ul>
<h3>Version 0.8.3</h3>
<ul>
Generic production of single phase R jobs
supporting installed on the fly libraries,
multiple R files including calling other binaries.
Jobs are run from dataset folders allowing
common files across jobs.
</ul>
<ul>
Generic production of a single Matlab job
from a single description file.
</ul>
<ul>
Generic production of a single binary job
from a single description file.
</ul>
<h3>Version 0.8.2</h3>
<ul>
All files which need to be written have been moved
to the same location as the index.html page to allow
write access outside the desired readonly area
where all the control scripts are. A Condor job
will want to edit to pull the current completed
job from the periodic plot and report list.
So continuous.cron and checkprogress.cron are moved.
</ul>
<h3>Version 0.8.1</h3>
<ul>
<li>
We now can craft a new index page for the site
upon adding a project to the file "projects.webinput"
which takes a list of departments and projects in this
form:<br>
Biochemistry - Attie,atti_yeast<br>
Limnology - GLEON,lake_study<br>
ECE - Van Veen,eeg_sim<br><br>
Note no space after the comma.
<br>
<li>
SOAR can now utilize "subdag external" place holders
allowing the postscript from the node before to create
a subdag to do a dynamic number of jobs in the next node.
<li>
Changing versions on the fly via version = xxxx in the
objconfig file was eliminated. This was to offer bias
to running 2 versions of the same project at the time
on the first data. The version of the control command 
will control where new binaries go.
<li>
GetSyncLock and ReleaseSyncLock were introduced as a way
to have only one prejob manage shared data for a dataset
of jobs and to do it reliably
</ul>
<h3>Version 0.8.0</h3>
<ul>
<li>
SOAR will now utilize global categories for DAG nodes
implemented on Condor V7.5.3. This allows creating 
a THROTTLES file in the glue script "jobs" folder.
Search for Throttles on this page. This is important 
if there are sheduler universe jobs within the workflow.
<li>
Major scripts no longer end in .pl.
<li>
Both cron scripts activated by condor are now Perl.
<li>
soar_version reports on software version of the 
install.
</UL>
<h3>Version 0.7.8</h3>
<ul>
<li>
Control, status, soar_rm and soar_to_condor replace
thier .pl counterparts.
<li>
Added a new feature to clean the current DONE
history for a version by allowing a --kind=reset
--pattern=XXXXXXX.
<li>
allow collecting accumulated CPU hours
from multiple nodes within a workflow.
<li>
allow plotting of multiple nodes within a 
complicated workflow by changing code to
parse a ',' separated list witten to file
PROFILE in "make_job_dag_text.pl".
<li>
Moved the DONE/RUNNING tracking to be version
based allowing runs which can be done with
2 versions at once and separately decide whats been done.
The first project run with the new code will take
the current history of what is done for itself
allowing a second version to run all the existing
data. Both will see new data for their version.
Good for automatic runs.
<li>
Fixed a ploting profile loss of data when there are
no image size resize events. Make sure when no
resize events happens, then the current time is used
to determine how long each job has been running.
Probably should do this even if there are resize 
events.
<li>
Fixed a bug where a missing version entry in the objconfig file
will cause repeated continuous processing of the Updating
because the files can not get to the correct location. "version"
in now considered mandatory.
<li>
--kind=oneoff renamed to --kind=reset cause it makes more sense.
<li>
--kind=nightly removed as --kind=new superceeds it.
<li>
A much reorganized version which delays setting up runs and consumming 
a jobid if not needed.
</ul>
<h3>Version 0.7.7</h3>
<ul>
<li>
Fixed an issue where it would die if it could not find 
the ENVVARS file for the last run.
<li>
soar_rm now correctly removes jobs from the running
jobs after it removes a job. Condor_rm should never 
be used.
<li>
Accumulated runtime is now updated as the report is run.
<li>
New systematic controls for storing, accessing and initializing 
Completed Jobs and Running Jobs since last --kind=reset
which resets to allow rerun of all jobs. In 7.6 and older
we could loose track and run extra jobs already completed
when running --kind=new in out automatic sweeps for new jobs
on 15 minute intervals
<li>
Profiling understands new Dagman dagman.ou format where
dates now have a 2 digit year at the beginning of each line
which had profiling broken for a bit.
</ul>

<h3>Version 0.7.6</h3>
<ul>
<li>
	Two new options add added to the automatic code updates.
	Look in that section for use of <b>whitelist</b> and 
	<b>dataset</b>.
<li>
	Projects running with datasets which have input data shared
	across all of the jobs in one dataset, but different from
	the contents of the same file for a different dataset can now
	be run routinely for updates and have the automatics look
	at only one dataset at a time. See "<b>--kind=DATASET</b> above.
<li>
	SOAR now uses a unique JOBID instead of the complicated
	run name.
<li> 
	SOAR will now look for new code and will replace it and if it is matlab
	re-compile a specified list of programs to make use of the new matlab files.
	See "<b>Automatic Code Replacement</b> above.
<li> 
	We no longer will place Matlab source files into runtime directory(*.m);
</ul>
<h3>Version 0.7.5</h3>
<ul>
<li> 
	Status will now create whitelists from selected sections of the results.
	See usage of status for details.
<li> 
	Control will ensure an entry in "checkprogress.cron" so the report
	and progress plot will exist even if multiple data sets are running at the same time
	for a single project. Previously only the most recent run got plots and reports.
<li>
	As reports are generated a constantly updated file in the project run contains a list
	of all the currently running jobs. Thus we can now do runs for --kind=new and we won't
	add any jobs which are still running. This means "continuous.cron" can be run frequently
	and look for new data to start avoiding the need to have someone start runs manually
	after all of the last run completed.
<li>
	A new tool, soar_rm removes a project run, removes the request for reports
	and plots for this run and removes the file which says these jobs are still
	running so the next --kind=new will restart them.
<li>
	Syntax for derived projects has changed to:
		./control --preproject=oldproject --preversion=oldversion --project=new name
		--version=newversion --install
</ul>

<A NAME="WantedFeatures"></A>
<h2>Wanted Features</h2>
<ul>
<li>
Allow better reporting on multiphase workflows
<li>
Allow better profiling plots as different phase are running
<li>
<li> 
<li>
<li>
Allow a project specific timeout after which time the still remaining
jobs could be restarted with a --kind=new. Most have storage location.
</ul>
</body>
</html>
