#! /usr/bin/env python
##**************************************************************
##
## Copyright (C) 2018, Condor Team, Computer Sciences Department,
## University of Wisconsin-Madison, WI.
##
## Licensed under the Apache License, Version 2.0 (the "License"); you
## may not use this file except in compliance with the License.  You may
## obtain a copy of the License at
##
##    http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.
##
##**************************************************************

import os
import time
import classad
import htcondor
from htcondor import JobEventType

# request a  personal condor with late materialization enabled
#testreq: personal
append_config = """
#<<CONDOR_TESTREQ_CONFIG;
  NUM_CPUS = 6
  NUM_SLOTS = 6
  SCHEDD_ALLOW_LATE_MATERIALIZE = true
  # turn on special logging of late materialization
  SCHEDD_MATERIALIZE_LOG = $(LOG)/MaterializeLog
  #ENABLE_ASYNC_LATE_MATERIALIZE_ITEM_DATA = true
CONDOR_TESTREQ_CONFIG
#endtestreq
"""

from pytest.Utils import Utils

testname = os.path.basename(__file__)[:-4]

sub = htcondor.Submit("""
	universe = vanilla
	executable = x_sleep.pl
	arguments = 10
	notification = never
	max_materialize = 4
	queue 10
	""")

logfile = testname + '.log'
sub['log'] = logfile

schedd = htcondor.Schedd()
with schedd.transaction() as txn:
	cluster = sub.queue(txn)

total_factories = 0
total_materialized = 0
total_exited = 0
total_running = 0
num_materialized = 0
num_running = 0
num_idle = 0
peak_materialized = 0
peak_running = 0
peak_idle = 0

#
# The globals are the reason we aren't doing cluster.WaitForRemoval().
#
def wait_for_cluster_removal(id, logfile, timeout):
	global total_factories
	global total_materialized
	global total_exited
	global total_running
	global num_materialized
	global num_running
	global num_idle
	global peak_materialized
	global peak_running
	global peak_idle

	start_time = time.time()
	end_time = start_time + timeout

	#
	# Process events one at a time until we run out.  Then block looking for
	# the next event until the deadline.
	#
	events = htcondor.JobEventLog(logfile)
	for event in events.events(timeout):
		if event.cluster != cluster:
			continue

		if event.type == JobEventType.CLUSTER_REMOVE:
			Utils.TLog("ulog: FactoryRemove({0})".format(cluster))
			total_factories -= 1

			print '\t          now peak total'
			print '\trunning  {0:4d} {1:4d} {2:5d}'.format(num_running, peak_running, total_running)
			print '\tidle     {0:4d} {1:4d} {2:5d}'.format(num_idle, peak_idle, total_materialized)
			print '\tpresent  {0:4d} {1:4d} {2:5d}'.format(num_materialized, peak_materialized, total_materialized)
			print '\tfactory  {0:4d} {1:4d} {2:5d}'.format(total_factories, 1, 1)
			break

		elif event.type == JobEventType.CLUSTER_SUBMIT:
			Utils.TLog("ulog: FactorySubmit({0})".format(cluster))
			total_factories += 1
			continue

		elif event.type == JobEventType.SUBMIT:
			total_materialized += 1
			num_materialized += 1
			num_idle += 1
			if num_materialized > peak_materialized : peak_materialized = num_materialized
			if num_idle > peak_idle : peak_idle = num_idle
			Utils.TLog("ulog: jobSubmit() idle={0}({1}), materialized={2}({3})".format(num_idle, peak_idle, num_materialized, peak_materialized))
			continue

		elif event.type == JobEventType.EXECUTE:
			total_running += 1
			num_running += 1
			num_idle -= 1
			if num_running > peak_running : peak_running = num_running
			Utils.TLog("ulog: jobExec() running={0}({1})".format(num_running, peak_running))
			continue

		elif event.type == JobEventType.JOB_TERMINATED:
			total_exited += 1
			num_running -= 1
			if event.TerminatedNormally : num_materialized -= 1
			Utils.TLog("ulog: jobTerm({0}) run={1}({2}), materialized={3}({4})".format(event.TerminatedNormally, num_running, peak_running, num_materialized, peak_materialized))
			continue

		elif event.type in [ JobEventType.IMAGE_SIZE ]:
			continue

		else:
			Utils.TLog("ulog: found unexpected event of type {0}, failing".format( str(event.type) ) );
			return 0
	else:
		return 1

	return 0
# end wait_for_cluster_removal

Utils.TLog("wait up to 5 min for jobs in cluster %d to materialize, run and exit" % cluster)
timed_out = wait_for_cluster_removal(cluster, logfile, 5*60)
if timed_out :
	Utils.TLog("Timed out waiting for cluster remove event")
	exit(1)

retval = 0
Utils.TLog("cluster %d completed, checking results:" % cluster)
if peak_materialized != int(sub['max_materialize']) :
	Utils.Log("FAILURE: peak_materialized({0}) != max_materialize({1})".format(peak_materialized, sub['max_materialize']))
	retval = 1
elif total_materialized != int(sub.getQArgs()) :
	Utils.Log("FAILURE: total_materialized({0}) != queue {1}".format(total_materialized, sub.getQArgs()))
	retval = 1
elif total_exited != int(sub.getQArgs()) :
	Utils.Log("FAILURE: total_exited({0}) != queue {1}".format(total_exited, sub.getQArgs()))
	retval = 1
else :
	Utils.Log("OK: all checks for cluster %d passed" % cluster)

# submit with more complex queue statement
#
sub = htcondor.Submit("""
	executable = x_sleep.pl
	arguments = 1
	notification = never	
	materialize_max_idle = 6
	My.Foo = "$(Item)"
	log = %s.$(ClusterId).log
	queue in (A, B, C, D, E, F, G, H, I, J, K, L)
	""" % testname)

num_items = 0
items = []
for item in iter(sub.itemdata()):
	items.append(item)
	num_items += 1

with schedd.transaction() as txn:
	cluster = sub.queue(txn)

logfile = "%s.%d.log" % (testname, cluster)

total_factories = 0
total_materialized = 0
total_exited = 0
total_running = 0
num_materialized = 0
num_running = 0
num_idle = 0
peak_materialized = 0
peak_running = 0
peak_idle = 0

Utils.TLog("wait up to 5 min for jobs in cluster %d to materialize, run and exit" % cluster)
timed_out = wait_for_cluster_removal(cluster, logfile, 5*60)
if timed_out:
	Utils.TLog("Timed out waiting for cluster remove event")
	exit(1)

Utils.TLog("cluster %d completed, checking results:" % cluster)
if peak_idle != int(sub['materialize_max_idle']):
	Utils.Log("FAILURE: peak_idle({0}) != materialize_max_idle({1})".format(peak_idle, sub['materialize_max_idle']))
	retval = 1
elif total_materialized != num_items:
	Utils.Log("FAILURE: total_materialized({0}) != {1}".format(total_materialized, num_items))
	retval = 1
elif total_exited != num_items:
	Utils.Log("FAILURE: total_exited({0}) != {1}".format(total_exited, num_items))
	retval = 1
else:
	Utils.Log("OK: total checks for cluster %d passed" % cluster)

# count jobs for this cluster in the history file
# and check to see that the Foo attribute is correct for each
#
Utils.TLog("Check history file for jobs of cluster %d:" % cluster)
num_ads = 0
num_ok = 0
ads = schedd.history("ClusterId==%d" % cluster, ['ProcId', 'Foo'])
for ad in ads:
	num_ads += 1
	if ad['Foo'] != items[ad['ProcId']]:
		Utils.Log("FAILURE: Value of Foo for Proc {0} is {1}, but it should be {2}".format(ad['ProcId'], ad['Foo'], items[ad['ProcId']]))
		retval = 1
	else:
		num_ok += 1

if num_ads != num_items:
	Utils.TLog("FAILURE: num_ads({0}) != num_items({1}) from history query".format(num_ads, num_items))
	retval = 1
elif num_ok == num_ads:
	Utils.TLog("OK: item checks for cluster %d passed" % cluster)

# submit using a python iterator
#
items = [
	{'item':'A', 'MY.Bar':'"a"', 'MY.Baz':'1'},
	{'item':'B', 'MY.Bar':'"b"', 'MY.Baz':'2'},
	{'item':'C', 'MY.Bar':'"c"', 'MY.Baz':'3'},
	{'item':'D', 'MY.Bar':'"d"', 'MY.Baz':'4'},
	{'item':'E', 'MY.Bar':'"e"', 'MY.Baz':'5'},
	{'item':'F', 'MY.Bar':'"f"', 'MY.Baz':'6'},
]
num_items = 6

with schedd.transaction() as txn:
	subres = sub.queue_with_itemdata(txn, 1, iter(items))

Utils.TLog("{0}".format(subres))

cluster = subres.cluster();
clusterad = subres.clusterad();
logfile = clusterad['UserLog']

total_factories = 0
total_materialized = 0
total_exited = 0
total_running = 0
num_materialized = 0
num_running = 0
num_idle = 0
peak_materialized = 0
peak_running = 0
peak_idle = 0

Utils.TLog("wait up to 5 min for jobs in cluster %d to materialize, run and exit" % cluster)
timed_out = wait_for_cluster_removal(cluster, logfile, 5*60)
if timed_out:
	Utils.TLog("Timed out waiting for cluster remove event")
	exit(1)

Utils.TLog("cluster %d completed, checking results:" % cluster)
if total_materialized != num_items:
	Utils.Log("FAILURE: total_materialized({0}) != {1}".format(total_materialized, num_items))
	retval = 1
elif total_exited != num_items:
	Utils.Log("FAILURE: total_exited({0}) != {1}".format(total_exited, num_items))
	retval = 1
else:
	Utils.Log("OK: total checks for cluster %d passed" % cluster)

# count jobs for this cluster in the history file
# and check to see that the Foo attribute is correct for each
#
Utils.TLog("Check history file for jobs of cluster %d:" % cluster)
num_ads = 0
num_ok = 0
ads = schedd.history("ClusterId==%d" % cluster, ['ProcId', 'Foo', 'Bar', 'Baz'])
for ad in ads:
	num_ads += 1
	item = items[ad['ProcId']]
	if ad['Foo'] != item['item']:
		Utils.Log("FAILURE: Value of Foo for Proc {0} is {1}, but it should be {2}".format(ad['ProcId'], ad['Foo'], item['item']))
		retval = 1
	elif ad['Bar'] != item['MY.Bar'].strip('"'):
		Utils.Log("FAILURE: Value of Bar for Proc {0} is {1}, but it should be {2}".format(ad['ProcId'], ad['Bar'], item['MY.Bar']))
		retval = 1
	elif ad['Baz'] != int(item['MY.Baz']):
		Utils.Log("FAILURE: Value of Baz for Proc {0} is {1}, but it should be {2}".format(ad['ProcId'], ad['Baz'], item['MY.Baz']))
		retval = 1
	else:
		num_ok += 1

if num_ads != num_items:
	Utils.TLog("FAILURE: num_ads({0}) != num_items({1}) from history query".format(num_ads, num_items))
	retval = 1
elif num_ok == num_ads:
	Utils.TLog("OK: item checks for cluster %d passed" % cluster)


exit(retval)

