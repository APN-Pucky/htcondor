
This chapter provides a basic, high-level overview of Condor, including
Condor's major features and limitations. Because Condor is a system to
implement a High-Throughput Computing environment, this section begins
defining what is meant by High-Throughput Computing.


\section{What is High-Throughput Computing (HTC) ?}

\index{Condor!overview|(}
\index{overview|(}
For many research and engineering projects, the quality of the research
or the product is heavily dependent upon the quantity of computing
cycles available. It is not uncommon to find problems that require weeks
or months of computation to solve. Scientists and engineers engaged in
this sort of work need a computing environment that delivers large
amounts of computational power over a long period of time. Such an
environment is called a High-Throughput Computing (HTC) environment.
\index{High-Throughput Computing (HTC)}
\index{HTC (High-Throughput Computing)}
In contrast, High Performance Computing (HPC)
\index{High-Performance Computing (HPC)}
\index{HPC (High-Performance Computing)}
environments deliver a
tremendous amount of compute power over a short period of time. HPC
environments are often measured in terms of FLoating point Operations
per Second (FLOPS). 
A growing community is not concerned about FLOPS,
as their problems are of a much larger scale.
These people are
concerned with floating point operations per month or per year.
They are
more interested in how many jobs they can complete over a long period of
time instead of how fast an individual job can complete.

\section{HTC and Distributed Ownership}

The key to HTC is to efficiently harness the use of all available
resources. Years ago, the engineering and scientific community relied on
large centralized mainframe and/or big-iron supercomputers to do
computational work. A large number of individuals and groups needed
to pool their financial resources to afford such a machine. Users had
to wait for their turn on the mainframe, 
and they only had a certain amount of time allocated to them.
While this environment was
inconvenient for users, it was very efficient, because the mainframe
was busy nearly all the time.

As computers became smaller, faster, and cheaper, users moved
away from centralized mainframes and purchased personal desktop
workstations and PCs. An individual or small group could afford a
computing resource that was available whenever they wanted it. The
personal computer was
usually far slower than the large centralized machine, but it
was worthwhile due to its
exclusive access.
Now, instead of one giant
computer for a large institution, there might be hundreds or thousands
of personal computers. This is an environment of distributed ownership,
\index{distributed ownership!of machines}
where individuals throughout an organization own their own resources.
The total computational power of the institution as a whole might rise
dramatically as the result of such a change, but because of distributed
ownership, individuals could not capitalize on the institutional growth of
computing power. And while distributed ownership is more convenient for
the users, it is much less efficient. Many personal desktop
machines sit idle for very long periods of time while their owners are
busy doing other things (such as being away at lunch, in meetings,
or at home sleeping). 

\section{\label{sec:what-is-condor}What is Condor ?}

Condor is a software system that creates a High-Throughput Computing
(HTC) environment by effectively harnessing the power of a cluster of
UNIX or NT workstations on a network. Although Condor can manage a
dedicated cluster of workstations, a key appeal of Condor is its
ability to effectively harness non-dedicated, preexisting resources in
a distributed ownership 
\index{distributed ownership!of machines}
setting such as machines sitting on people's desks
in offices and labs. 

\subsection{A Hunter of Available Workstations}

Instead of running a CPU-intensive job in the background on their own
workstation, a user submits their job to Condor. Condor finds an
available machine on the network and begins running the job on
that machine. When Condor detects that a machine running a Condor job
is no longer available (perhaps because the owner of the machine
came back from lunch and started typing on the keyboard), Condor
checkpoints 
\index{checkpoint}
the job and migrates 
\index{migration}
it over the network to a
different machine which would otherwise be idle. Condor restarts
the job on the new machine to continue from precisely where it left off.
If no
machine on the network is currently available, then the job is stored in
a queue on disk until a machine becomes available.

As an example, a compute job that typically takes 5
hours to run is submitted to Condor.
Condor may start running the job on Machine A, but after 3
hours Condor detects activity on the keyboard.
Condor will checkpoint
the job and migrates it to Machine B.
After two hours on Machine B,
the job completes (notifying the submitter via e-mail). 

Perhaps this 5 hour compute job must be run 250 different times
\index{job!multiple data sets}
(perhaps on 250 different data sets).
In this case, Condor can be a real time saver.
With one command, all 250 runs are submitted to Condor.
Depending upon the number of machines in the organization's Condor
pool, there could be dozens or even hundreds of otherwise idle machines
running the job at any given moment.

Condor makes it easy to maximize the number of machines which can run
a job. Because Condor does not require participating machines to
share file systems (via NFS or AFS for example), machines across the
entire enterprise can run a job, including machines in different
administrative domains.
Condor does not require an
account (login) on machines where it runs a job.
Condor can do this
because of its \Term{remote system call}
\index{remote system call}
technology, which traps
operating system calls for such operations as reading or writing from disk
files and sends them back over the network to be performed on the machine
where the job was submitted. 

\subsection{Effective Resource Management}

In addition to migrating jobs to available machines, Condor provides
sophisticated and distributed resource management.
\index{Condor!resource management}
\index{resource!management}
Match-making resource
\index{matchmaking}
owners with resource consumers is the cornerstone of a successful HTC
environment.
Unlike many other compute cluster resource management
systems which attach properties to the job queues themselves (resulting
in user confusion over which queue to use as well as administrative
hassle in constantly adding and editing queue properties to satisfy user
demands), Condor implements a clean design called \Term{ClassAds}. 
\index{ClassAd}

ClassAds work in a fashion similar to the newspaper classified
advertising want-ads. All machines in the Condor pool advertise their
resource properties, such as available RAM memory, CPU type and speed,
virtual memory size, physical location, current load average, and many
other static and dynamic properties, into a \Term{resource offer} ad.
\index{resource!offer}
Likewise,
when submitting a job, users can specify a \Term{resource request} ad
\index{resource!request}
which
defines both the required and a desired set of properties to run the job.
Similarly, a resource offer ad can define requirements and preferences.
Condor acts as a broker by matching and ranking resource
offer ads with resource request ads, making certain that all
requirements in both ads are satisfied. During this match-making
process, Condor also takes several layers of priority values into
consideration: the priority the user assigned to the resource request
ad, the priority of the user which submitted the ad, and desire of
machines in the pool to accept certain types of ads over others. 

\section{Distinguishing Features}

\begin{description}
	\item[Checkpoint and Migration.] Users of Condor may be assured that
their jobs will eventually complete even in an opportunistic computing
environment. If a user submits a job to Condor which runs on another's
workstation, but the job is not finished when the workstation
owner returns, the job can be checkpointed.
\index{checkpoint}
The job continues
by migrating 
\index{migration}
to another machine. It makes progress toward
its completion by the checkpoint and migration feature.
Condor's periodic checkpoint feature 
\index{checkpoint!periodic}
periodically checkpoints a job even in lieu of migration in order to
safeguard the accumulated computation time on a job from being lost in the
event of a system failure such as the machine being shutdown or a crash.
	\item[Remote System Calls.] 
\index{remote system call}
Despite running jobs on remote machines,
the Condor standard universe execution
mode preserves the local execution environment
via remote system calls. Users do not have to worry
about making data files available to remote workstations or even
obtaining a login account on remote workstations before Condor executes
their programs there. The program behaves under Condor as if it were
running as the user that submitted the job on the workstation where it
was originally submitted, no matter on which machine it really ends up
executing on.
	\item[No Changes Necessary to User's Source Code.] No special
programming is required to use Condor. Condor is able to run normal
non-interactive UNIX or NT programs.  The checkpoint and migration of
programs by Condor is transparent and automatic, as is the use of
remote system calls.  If these facilities are desired, the user only
re-links the program.  The code is not compiled or changed.
	\item[Sensitive to the Desires of Workstation Owners.] The
owner of a
workstation has complete priority over the workstation,
by default.
A workstation owner is generally happy to let others compute on
the workstation while it is idle, but wants the workstation back
promptly upon returning. The owner does not want to take special
action to regain control. Condor handles this automatically. 
	\item[ClassAds.]The ClassAd mechanism 
\index{ClassAd}
in Condor provides an extremely
flexible, expressive framework for matchmaking
resource requests with resource offers. One result is that users can
easily request practically any resource, both in terms of job
requirements and job desires.
For example, a user can require that a job run on a machine
with 64 Mbytes of RAM,
but state a preference for 128 Mbytes if available.
Likewise, a workstation
can state a preference in a resource offer to run jobs
from a specified set of users, and it can require that there be
no interactive workstation
activity detectable between 9 am and 5 pm before starting a job.
Job requirements/preferences and resource availability constraints can be
described in terms of powerful expressions, resulting in
Condor's adaptation to nearly any desired policy. 
\end{description}
\index{Condor!overview|)}
\index{overview|)}

\section{\label{sec:current-limitations}Current Limitations}

\begin{description}

\index{Condor!limitations, under UNIX}
	\item[Limitations on Jobs which can Checkpointed] Although Condor can schedule and
run any type of process, Condor does have some limitations on jobs that it can
transparently checkpoint and migrate:

\input{user-man/limitations.tex}

	Note: these limitations \emph{only} apply to jobs which Condor
has been asked to transparently checkpoint.  If job checkpointing is not
desired, the limitations above do not apply.

	\item[Security Implications.] Condor does a significant amount of work to prevent 
security hazards, but loopholes are known to exist.  Condor can be instructed 
to run user programs only as the UNIX user nobody, a user login which traditionally has very 
restricted access.  But even with access solely as user nobody, a sufficiently 
malicious individual could do such things as fill up /tmp (which is world writable) and/or gain 
read access to world readable files.
Furthermore, where the security of machines in the pool is a high concern, 
only machines where the UNIX user root on that machine can be trusted should be admitted
into the pool. Condor provides the administrator with IP-based security mechanisms 
to enforce this.

	\item[Jobs Need to be Re-linked to get Checkpointing and Remote System Calls] Although 
typically no source code changes are required,
Condor requires
that the jobs be re-linked with the Condor libraries to take
advantage of checkpointing and remote system calls. This often
precludes commercial software binaries from taking advantage of these services
because commercial packages rarely make their object code
available. 
Condor's other services are still available for these commercial packages.

\end{description}

\section{\label{sec:Availability}Availability}
\index{Condor!availability}
Condor is currently available as a free download from the Internet via the World Wide Web at  
URL \Url{http://www.cs.wisc.edu/condor/downloads}.
Binary distributions of Condor version 6.x are available for the platforms 
detailed in Table~\ref{supported-platforms}.  A platform is an 
architecture/operating system combination.  
Condor binaries are available most major versions of UNIX, as well as
Windows NT.  

In the table, \Term{clipped} means that Condor does not support
checkpointing or remote system calls on the given platform. 
This means that \Term{standard} jobs are not supported, only
\Term{vanilla} jobs.
See section~\ref{sec:Choosing-Universe} on
page~\pageref{sec:Choosing-Universe} for more details on job universes
within Condor and their abilities and limitations.

The Condor source code is no longer available for public download from the Internet.  If you 
desire the Condor source code, please contact the Condor Team in order to discuss it further 
(see Section~\ref{contact-info}, on page~\pageref{contact-info}).

\begin{center}
\begin{table}[hbt]
\begin{tabular}{|ll|} \hline
\emph{Architecture} & \emph{Operating System} \\ \hline \hline
Hewlett Packard PA-RISC (both PA7000 and PA8000 series) & HPUX 10.20 \\ \hline
Sun SPARC Sun4{m,c}, Sun UltraSPARC & Solaris 2.5.x, 2.6, 2.7 \\ \hline
Silicon Graphics MIPS (R4400, R4600, R8000, R10000) & IRIX 6.2, 6.3, 6.4 \\ 
 & IRIX 6.5 \\ \hline
Intel x86 & Linux 2.0.x, 2.2.x, glibc20, glibc21, libc5 \\
 & Solaris 2.5.x, 2.6, 2.7 \\ 
 & Windows NT 4.0 (``clipped'') \\ \hline
Digital ALPHA & OSF/1 (Digital Unix) 4.x \\
 & Linux 2.0.x, Linux 2.2.x (``clipped'') \\ \hline
\end{tabular}
\caption{\label{supported-platforms}Condor \VersionNotice\ supported platforms}
\end{table}
\end{center}

\input{overview/version-history.tex}

\section{\label{contact-info}Contact Information}

\index{Condor!contact information}
The latest software releases, publications/papers regarding Condor and other 
High-Throughput Computing
research can be found at the official web site for Condor at  
\Url{http://www.cs.wisc.edu/condor}.

In addition, there is an email listgroup at \Url{mailto:condor-world@cs.wisc.edu}.  The Condor Team 
uses this email listgroup to announce new releases of Condor and other major Condor-related 
news items.  Membership into condor-world is automated by MajorDomo software.  To 
subscribe or unsubscribe from the the list, follow the instructions at  
\Url{http://www.cs.wisc.edu/condor/condor-world/condor-world.html}.  Because many of us receive 
too much email as it is, you'll be happy to know that the condor-world email listgroup is 
moderated and only major announcements of wide interest are distributed.

Finally, you can reach the Condor Team directly.  The Condor Team is comprised of the 
developers and administrators of Condor at the University of Wisconsin-Madison. Condor 
questions, comments, pleas for help, and requests for commercial contract consultation or support 
are all welcome; just send Internet email to \Url{mailto:condor-admin@cs.wisc.edu}.  Please include your 
name, organization, and telephone number in your message.  If you are having trouble with 
Condor, please help us troubleshoot by including as much pertinent information as you can, 
including snippets of Condor log files. 


