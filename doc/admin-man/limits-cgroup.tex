%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\label{sec:Resource-Limits-Cgroup}Limiting Resource Usage using cgroups} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\index{resource limits with cgroups}
\index{limits!on resource usage with cgroup}

While the method desribed above to limit a job's resource usage is portable,
and should run on any Linux or BSD or Unix system, it suffers from one
large flaw.  That is, the resource limits imposed are per process, not per job.
An HTCondor job is often composed of many Unix processes.  If the above 
method is used to impose a 2 Gigabyte memory limit, that limit applies to
each process in the job individually.  If a job created 100 processes, each just under 2 Gigabytes, the job would continue without the resource limits 
kicking in.  Clearly, this is not what the machine owner intends.  Moreover,
the memory limit only applies to the virtual memory size, not the physical
memory size, or the resident set size.  This can be a problem for jobs
that use the mmap system call to map in a large chunk of virtual memory, but
only need a small amount of it at once.  Typically, the resource the
administrator would like to control is physical memory, for when that is in
short supply, the machine starts paging, and can become unresponsive very
quickly.

Starting in HTCondor version 7.9.1, the \Condor{starter} can, using the 
Linux cgroup capability, apply resource limits collectively
to sets of jobs, and to applies limits to the physical memory used by a set
of processes.  The main downside of this technique is that it is only 
available on relatively new Unix distributions, such as RHEL 6 and Debian 6.
Also, it may require editing of system configuration files.

To enable cgroup based limits, first enable cgroup based measuring, as
described in section ~\ref{sec:CGroupTracking}.  Once that is set,
the \Condor{starter} will create a cgroup for each job, and set two
attributes in that cgroup which control resource usage therein.  These
two attributes are the cpu.shares attribute in the cpu controller, and one
of two attributes in the memory controller, either memory.limit\_in\_bytes, or 
memory.soft\_limit\_in\_bytes.  The parameter \Macro{MEMORY\_LIMIT} controls
wether the hard limit (the former) or the soft limit will be used.  If
\Macro{MEMORY\_LIMIT} is set to the string "hard", the hard limit will be
used, if set to "soft", the soft limit will be used.  Otherwise, no
limit will be set if the value is "none".  The default is "none".  If
the hard limit is in force, then the total amount of physical memory
used by the sum of all processes in this job will not be allowed to exceed
the limit.  If the processes try to allocate more memory, the allocation will
succeed, and virtual memory will be allocated, but no additional physical memory,
the system will keep the amount of physical memory constant by swapping some
page from that job out of memory.  However, if the soft limit is in place,
the job will be allowed to go over the limit, if there is free memory 
available on the system.  Only when there is contention between other processes for physical memory, will the system force physical memory into swap, and push
the physical memory used towards the assigned limit.  The memory size used in both cases is the MEMORY attribute of the machine ad.  Note that MEMORY is 
a static amount when using static slots, but is dynamic when partitionable 
slots are used.

In addition to memory, the starter can also control the total amount of 
CPU used by all processes within a job.  To do this, it writes a value
to the cpu.shares attribute of the cgroup cpu controller.  The value
is writes is copies from the \Macro{Cpus} attribute of the machine slot
ad.  Again, like the memory attribute, this value is fixed for static slots,
but dynamic under partitionable slots.  This tells the operating system
to assign cpu usage proportionally to the number of cpus in the slot.  Unlike
memory, there is no concept of "soft" or "hard", this limit only applies when
there is contention for cpu.  That is, on an eight core machine, with
only a single, one-core slot running, and otherwise idle, the job running
in the one slot could consume all eight cpus concurrently with this limit
in play, if it is the only thing running.  If, however, all eight slots where
running jobs, and each configured for one cpu, the cpu usage would be assigned
equally to each job, regardless of the number of processes in each job.


