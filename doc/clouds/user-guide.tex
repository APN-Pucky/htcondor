\section{\label{sec:clouds-annex}HTCondor Annex User's Guide}

A user of \Condor{annex} may be a regular job submitter, or she may be an
HTCondor pool administrator.  This guide will cover basic \Condor{annex} usage
first, followed by advanced usage that may be of less interest to the
submitter.  Users interested in customizing \Condor{annex} should consult
section \ref{sec:clouds-services}.

\subsection{Considerations and Limitations}

When you run \Condor{annex}, you are adding (virtual) machines to an HTCondor
pool.  As a submitter, you probably don't have permission to add machines to
the HTCondor pool you're already using; generally speaking, security concerns
will forbid this.  If you're a pool administrator, you can of course add
machines to your pool as you see fit.  By default, however, \Condor{annex}
instances will only start jobs submitted by the user who started the annex,
so pool administrators using \Condor{annex} on their users' behalf will
probably want to use the \Opt{-owners} option or \Opt{-no-owner} flag;
see the man page (section \ref{man-condor-annex}).  Once the new machines
join the pool, they will run jobs as normal.

Submitters, however, will have to set up their own personal HTCondor pool,
so that \Condor{annex} has a pool to join, and then work with their pool
administrator if they want to move their existing jobs to their new pool.
Otherwise, jobs will have to be manually divided (removed from one and
resubmitted to the other) between the pools.  For instructions on creating
a personal HTCondor pool, preparing an AWS account for use by \Condor{annex},
and then configuring \Condor{annex} to use that account, see
section~\ref{sec:clouds-annex-first-time}.

Starting in v8.7.1, \Condor{annex} will check for inbound access to the
collector (usually port 9618) before starting an annex (it does not
support other network topologies).  When checking connectivity
from AWS, the IP(s) used by the AWS Lambda function implementing this check
may not be in the same range(s) as those used by AWS instance; please
consult AWS's list of all their IP
ranges\footnote{\URL{https://ip-ranges.amazonaws.com/ip-ranges.json}}
when configuring your firewall.

Starting in v8.7.2, \Condor{annex} requires that the AWS secret (private) key file
be owned by the submitting user and not readable by anyone else.  This
helps to ensure proper attribution.

\subsection{Basic Usage}

%% A quick-start guide with basic usage information is kept on our Wiki:
%%
%% This Wiki page should be frozen and migrated into the manual for
%% the stable release (v8.8.0); ideally, this would also be a reasonable
%% thing to do for the next development series (v8.9) as well.
%%
%% \URL{https://htcondor-wiki.cs.wisc.edu/index.cgi/wiki?p=HowToUseCondorAnnexWithOnDemandInstancesEightSevenFour}.

This section assumes you're logged into a Linux machine an that you've
already configured \Condor{annex}.  If you haven't, see
section~\ref{sec:clouds-annex-first-time}.

All the terminal commands (shown in a box without a title) and file edits
(shown in a box with an \emph{emphasized filename} for a title) in this
section take place on the Linux machine.  In this section, we follow the
common convention that the commands you type are preceded by by `\$'
to distinguish them from any expected output; don't copy that part of each
of the following lines.  (Lines which end in a `\textbackslash' continue on
the following line; be sure to copy both lines.  Don't copy the
`\textbackslash' itself.)

\subsubsection{What You'll Need to Know}

To create a HTCondor annex with on-demand instances, you'll need to know two
things:

\begin{enumerate}
\item A name for it.  ``MyFirstAnnex'' is a fine name for your first annex.
\item How many instances you want.  For your first annex, when you're checking to make sure things work, you may only want one instance.
\end{enumerate}

\subsection{Start an Annex}

Entering the following command will start an annex named ``MyFirstAnnex'' with
one instance.  \Condor{annex} will print out what it's going to do, and then
ask you if that's OK.  You must type `yes' (and hit enter) at the prompt to
start an annex; if you do not, \Condor{annex} will print out instructions
about how to change whatever you may not like about what it said it was going
to do, and then exit.

\terminal{%
\$ condor\_annex -count 1 -annex-name MyFirstAnnex \\
Will request 1 m4.large on-demand instance for 0.83 hours.  Each instance will \\
terminate after being idle for 0.25 hours. \\
Is that OK?  (Type 'yes' or 'no'): yes \\
Starting annex... \\
Annex started.  Its identity with the cloud provider is \\
'TestAnnex0\_f2923fd1-3cad-47f3-8e19-fff9988ddacf'.  It will take about three \\
minutes for the new machines to join the pool.
}

You won't need to know the annex's identity with the cloud provider unless
something goes wrong.

Before starting the annex, \Condor{annex} (v8.7.1 and later) will check to make sure that the
instances will be able to contact your pool.  Contact the Linux machine's
administrator if \Condor{annex} reports a problem with this step.

\subsubsection{Instance Types}

Each instance type provides a different number (and/or type) of CPU cores,
amount of RAM, local storage, and the like.  We recommend starting with
`m4.large', which has 2 CPU cores and 8 GiB of RAM, but you can see the
complete list of instance types at the following URL:\\
\\
\url{https://aws.amazon.com/ec2/instance-types/}\\
\\
You can specify an instance type with
the \texttt{-aws-on-demand-instance-type} flag.

\subsubsection{Leases}

By default, \Condor{annex} arranges for your annex's instances to be terminated
after \texttt{0.83} hours (50 minutes) have passed.  Once it's in place, this
lease doesn't depend on the Linux machine, but it's only checked every five
minutes, so give your deadlines a lot of cushion to make you don't get charged
for an extra hour.  The lease is intended to help you conserve money by
preventing the annex instances from accidentally running forever.
You can specify a lease duration (in decimal hours)
with the \texttt{-duration} flag.

\begin{samepage}
If you need to adjust the lease for a particular annex, you may do so by
specifying an annex name and a duration, but not a count.  When you do so,
the new duration is set starting at the current time.  For example, if you'd
like ``MyFirstAnnex'' to expire eight hours from now:

\terminal{%
\$ condor\_annex -annex-name MyFirstAnnex -duration 8 \\
Lease updated.
}
\end{samepage}

\subsubsection{Idle Time}

By default, \Condor{annex} will configure your annex's instances to terminate
themselves after being idle for \texttt{0.25} hours (fifteen minutes).  This
is intended to help you conserve money in case of problems or an extended
shortage of work.  As noted in the example output above, you can specify a max
idle time (in decimal hours) with the \texttt{-idle} flag.  \Condor{annex}
considers an instance idle if it's unclaimed (see
section~\ref{state:unclaimed} for a definition), so it won't get tricked by
jobs with long quiescent periods.

\subsubsection{Multiple Annexes}

You may have up to fifty (or fewer, depending what else you're doing with your
AWS account) differently-named annexes running at the same time.  Running
\Condor{annex} again with the same annex name before stopping that annex will
both add instances to it and change its duration.  Only instances which start
up after an invocation of \Condor{annex} will respect that invocation's max
idle time.  That may include instances still starting up from your previous
(first) invocation of \Condor{annex}, so be sure your instances have all
joined the pool before running \Condor{annex} again with the same annex name
if you're changing the max idle time.  Each invocation of \Condor{annex}
requests a certain number of instances of a given type; you may specify
the instance type, the count, or both with each invocation, but doing so
does not change the instance type or count of any previous request.

\subsection{Monitor your Annex}

\begin{samepage}
You can find out if an instance has successfully joined the pool in the
following way:

{\obeyspaces\terminal{%
\$ condor\_annex status \\
Name                               OpSys      Arch   State     Activity     Load \\
\\
slot1@ip-172-31-48-84.ec2.internal LINUX      X86\_64 Unclaimed Benchmarking  0.0 \\
slot2@ip-172-31-48-84.ec2.internal LINUX      X86\_64 Unclaimed Idle          0.0 \\
\\
\hphantom{xxxxxxxxxxxxxxx}Total Owner Claimed Unclaimed Matched Preempting Backfill  Drain \\
\\
\hphantom{xx}X86\_64/LINUX     2     0       0         2       0          0        0      0 \\
\hphantom{xxxxxxxxx}Total     2     0       0         2       0          0        0      0
}}
\end{samepage}

This example shows that the annex instance you requested has joined your pool.
(The default annex image configures one static slot for each CPU it finds on
start-up.)

\begin{samepage}
You may instead use \Condor{status}:

{\obeyspaces\terminal{%
\$ condor\_status -annex MyFirstAnnex \\
slot1@ip-172-31-48-84.ec2.internal  LINUX     X86\_64 Unclaimed Idle 0.640 3767 \\
slot2@ip-172-31-48-84.ec2.internal  LINUX     X86\_64 Unclaimed Idle 0.640 3767 \\
\\
\hphantom{xxxxxxxxxxxxxx} Total Owner Claimed Unclaimed Matched Preempting Backfill  Drain \\
\hphantom{xx}X86\_64/LINUX     2     0       0         2       0          0        0      0 \\
\hphantom{xxxxxxxxx}Total     2     0       0         2       0          0        0      0
}}
\end{samepage}

\begin{samepage}
You can also get a report about the instances which have not joined your pool:

{\obeyspaces\terminal{%
\$ condor\_annex -annex MyFirstAnnex -status \\
STATE          COUNT \\
pending            1 \\
TOTAL              1
\\
Instances not in the pool, grouped by state: \\
pending i-06928b26786dc7e6e
}}
\end{samepage}

\subsubsection{Multiple Annexes}

The following command reports on all annex instance which have joined the pool,
regardless of which annex they're from:

{\obeyspaces\terminal{%
\$ condor\_status -annex \\
slot1@ip-172-31-48-84.ec2.internal  LINUX     X86\_64 Unclaimed Idle 0.640 3767 \\
slot2@ip-172-31-48-84.ec2.internal  LINUX     X86\_64 Unclaimed Idle 0.640 3767 \\
slot1@ip-111-48-85-13.ec2.internal  LINUX     X86\_64 Unclaimed Idle 0.640 3767 \\
slot2@ip-111-48-85-13.ec2.internal  LINUX     X86\_64 Unclaimed Idle 0.640 3767 \\
\\
\hphantom{xxxxxxxxxxxxxxx}Total Owner Claimed Unclaimed Matched Preempting Backfill  Drain \\
\hphantom{xx}X86\_64/LINUX     4     0       0         4       0          0        0      0 \\
\hphantom{xxxxxxxxx}Total     4     0       0         4       0          0        0      0
}}

\begin{samepage}
The following command reports about instance which have not joined the pool,
regardless of which annex they're from:

{\obeyspaces\terminal{%
\$ condor\_annex -status \\
NAME                        TOTAL running \\
NamelessTestA                   2       2 \\
NamelessTestB                   3       3 \\
NamelessTestC                   1       1 \\
\\
NAME                        STATUS  INSTANCES... \\
NamelessTestA               running i-075af9ccb40efb162 i-0bc5e90066ed62dd8 \\
NamelessTestB               running i-02e69e85197f249c2 i-0385f59f482ae6a2e \\ i-06191feb755963edd \\
NamelessTestC               running i-09da89d40cde1f212
}}
\end{samepage}

The ellipsis in the last column (\texttt{INSTANCES...}) is to indicate that
it's a very wide column and may wrap (as it has in the example),
not that it has been truncated.

The following command combines these two reports:

{\obeyspaces\terminal{%
\$ condor\_annex status \\
Name                               OpSys      Arch   State     Activity     Load \\
\\
slot1@ip-172-31-48-84.ec2.internal LINUX      X86\_64 Unclaimed Benchmarking  0.0 \\
slot2@ip-172-31-48-84.ec2.internal LINUX      X86\_64 Unclaimed Idle          0.0 \\
\\
\hphantom{xxxxxxxxxxxxxxx}Total Owner Claimed Unclaimed Matched Preempting Backfill  Drain \\
\\
\hphantom{xx}X86\_64/LINUX     2     0       0         2       0          0        0      0 \\
\hphantom{xxxxxxxxx}Total     2     0       0         2       0          0        0      0 \\
\\
Instance ID         not in Annex  Status  Reason (if known) \\
i-075af9ccb40efb162 NamelessTestA running - \\
i-0bc5e90066ed62dd8 NamelessTestA running - \\
i-02e69e85197f249c2 NamelessTestB running - \\
i-0385f59f482ae6a2e NamelessTestB running - \\
i-06191feb755963edd NamelessTestB running - \\
i-09da89d40cde1f212 NamelessTestC running -
}}

\subsection{Run a Job}

Starting in v8.7.1, the default behaviour for an annex instance is to run only
jobs submitted by the user who ran the \Condor{annex} command.  If you'd
like to allow other users to run jobs, list them (separated by commas; don't forget
to include yourself) as arguments to the \texttt{-owner} flag when you start
the instance.  If you're creating an annex for general use, use the
\texttt{-no-owner} flag to run jobs from anyone.

Also starting in v8.7.1, the default behaviour for an annex instance is to run
only jobs which have the \texttt{MayUseAWS} attribute set (to true).  To
submit a job with \texttt{MayUseAWS} set to true, add
\texttt{+MayUseAWS = TRUE} to the submit file somewhere before the
\texttt{queue} command.  To allow an existing job to run in the annex,
use \texttt{condor\_q\_edit}.  For instance, if you'd like cluster 1234 to run
on AWS:

\terminal{%
\$ condor\_qedit 1234 "MayUseAWS = TRUE" \\
Set attribute "MayUseAWS" for 21 matching jobs.
}

\subsection{Stop an Annex}

The following command shuts HTCondor off on each instance in the annex; if
you're using the default annex image, doing so causes each instance to shut
itself down.  HTCondor does \emph{not} provide a direct method terminating
\Condor{annex} instances.

\terminal{%
\$ condor\_off -annex MyFirstAnnex \\
Sent "Kill-Daemon" command for "master" to master ip-172-31-48-84.ec2.internal
}

\subsubsection{Multiple Annexes}

The following command turns off all annex instances in your pool, regardless
of which annex they're from:

\terminal{%
\$ condor\_off -annex \\
Sent "Kill-Daemon" command for "master" to master ip-172-31-48-84.ec2.internal \\
Sent "Kill-Daemon" command for "master" to master ip-111-48-85-13.ec2.internal
}

\subsection{Using Different or Multiple AWS Regions}

It sometimes advantageous to use multiple AWS regions, or convenient to use
an AWS region other than the default, which is \Expr{us-east-1}).  To change
the default, set the configuration macro \Macro{ANNEX\_DEFAULT\_AWS\_REGION}
to the new default.  (If you used the \Condor{annex} automatic setup, you
can edit the \Expr{user\_config} file in \Expr{.condor} directory in
your home directory.)  Once you do this, you'll have to re-do the setup,
as setup is region-specific.

If you'd like to use multiple AWS regions, you can specify which reason to use
on the command line with the \Opt{-aws-region} flag.  Each region may have
zero or more annexes active simultaneously.

\subsection{Advanced Usage}

The previous section covered using what AWS calls ``on-demand''
instances.  (An ``instance'' is ``a single occurrence of something,'' in
this case, a virtual machine.  The intent is to distinguish between the
active process that's pretending to be a real piece of hardware --
the ``instance'' -- and the template it used to start it up, which may also
be called a virtual machine.)  An on-demand instance has a price fixed by AWS;
once acquired, AWS will let you keep it running as long as you continue to
pay for it.

In constrast, a ``Spot'' instance has a price determined by an (automated)
auction; when you request a ``Spot'' instance, you specify the most (per hour)
you're willing to pay for that instance.  If you get an instance, however,
you pay only what the spot price is for that instance; in effect, AWS
determines the spot price by lowering it until they run out of instances
to rent.  AWS advertises savings of up to 90\% over on-demand instances.

There are two drawbacks to this cheaper type of instance: first,
you may have to wait (indefinitely) for instances to become available at
your preferred price-point; the second is that your instances may be taken
away from you before you're done with them because somebody else will pay
more for them.  (You won't be charged for the hour in which AWS kicks
you off an instance, but you will still owe them for all of that instance's
previous hours.)  Both drawbacks can be mitigated (but not eliminated) by
bidding the on-demand price for an instance; of course, this also minimizes
your savings.

Determining an appropriate bidding strategy is outside the purview of
this manual.

\subsubsection{Using AWS Spot Fleet}

\Condor{annex} supports Spot instances via an AWS technology called
``Spot Fleet''.  Normally, when you request instances, you request a specific
type of instance (the default on-demand instance is, for instance, `m4.large'.)
However, in many cases, you don't care too much about how many cores an
intance has -- HTCondor will automatically advertise the right number and
schedule jobs appropriately, so why would you?  In such cases -- or in
other cases where your jobs will run acceptably on more than one type of
instance -- you can make a Spot Fleet request which says something like
``give me a thousand cores as cheaply as possible'', and specify that
an `m4.large' instance has two cores, while `m4.xlarge' has four, and so
on.  (The interface actually allows you to assign arbitrary values --
like HTCondor slot weights -- to each instance
type\footnote{Strictly speaking, to each ``launch specification''; see
the explanation below, in the section \emph{AWS Instance User Data}.},
but the default value is core count.)  AWS will then divide the current price for each
instance type by its core count and request spot instances at the cheapest
per-core rate until the number of cores (not the number of instances!) has
reached a thousand, or that instance type is exhausted, at which point it will
request the next-cheapest instance type.

(At present, a Spot Fleet only chooses the cheapest price within each
AWS region; you would have to start a Spot Fleet in each AWS region you
were willing to use to make sure you got the cheapest possible price.  For
fault tolerance, each AWS region is split into independent zones, but each
zone has its own price.  Spot Fleet takes care of that detail for you.)

In order to create an annex via a Spot Fleet, you'll need a file containing
a JSON blob which describes the Spot Fleet request you'd like to make.  (It's
too complicated for a reasonable command-line interface.)  The AWS web
console can be used to create such a file; the button to download that
file is (currently) in the upper-right corner of the last page before
you submit the Spot Fleet request; it is labeled `JSON config'.  You
may need to create an IAM role the first time you make a Spot Fleet
request; please do so before running \Condor{annex}.

You \emph{must} select the instance role profile used by your on-demand
instances for \Condor{annex} to work.  This value will have been stored in the
configuration macro \Macro{ANNEX\_DEFAULT\_ODI\_INSTANCE\_PROFILE\_ARN}
by the setup procedure.

%% I should really fix it so that the ODI InstanceProfileARN is preferred
%% over the IamInstanceProfile specified in the JSON configuration file.

Specify the JSON configuration file using \Opt{-aws-spot-fleet-config-file},
or set the configuration macro \Macro{ANNEX\_DEFAULT\_SFR\_CONFIG\_FILE} to
the full path of the file you just downloaded, if you'd like it to become
your default configuration for Spot annexes.  Be aware that \Condor{annex}
does \emph{not} alter the validity period if one is set in the Spot
Fleet configuration file.  You should remove the references to `ValidFrom'
and `ValidTo' in the JSON file to avoid confusing surprises later.

Additionally, be aware that \Condor{annex} uses the Spot Fleet API in
its ``request'' mode, which means that an annex created with Spot
Fleet has the same semantics with respect to replacement as it would
otherwise: if an instance terminates for any reason, including AWS
taking it away to give to someone else, it is not replaced.

You must specify the number of cores (total instance weight; see above) using
\Opt{-slots}.  You may also specify \Opt{-aws-spot-fleet}, if you wish;
doing so may make this \Condor{annex} invocation more self-documenting.
You may use other options as normal, excepting those which begin with
\Opt{-aws-on-demand}, which indicates an option specific to on-demand
instances.

\subsubsection{Custom HTCondor Configuration}

When you specify a custom configuration, you specify the full path to a
configuration directory which will be copied to the instance.  The customizations
performed by \Condor{annex} will be applied to a temporary copy of this
directory before it is uploaded to the instance.  Those customizations
consist of creating two files: {\tt password\_file.pl} (named that way to ensure
that it isn't ever accidentally treated as configuration), and
{\tt 00ec2-dynamic.config}.  The former is a password file for use by the pool
password security method, which if configured, will be used by \Condor{annex}
automatically.  The latter is an HTCondor configuration file; it is named
so as to sort first and make it easier to over-ride with whatever configuration
you see fit.

\subsubsection{AWS Instance User Data}

HTCondor doesn't interfere with this in any way, so if you'd like to set
an instance's user data, you may do so.  However, as of v8.7.2, the
\Opt{-user-data} options don't work for on-demand instances (the default
type).  If you'd like to specify user data for your Spot Fleet -driven
annex, you may do so in four different ways: on the command-line or
from a file, and for all launch specifications or for only those launch
specifications which don't already include user data.  These two choices
correspond to the absence or presence of a trailing \Opt{-file} and the
absence or presence of \Opt{-default} immediately preceding \Opt{-user-data}.

A ``launch specification,'' in this context, means one of the virtual machine
templates you told Spot Fleet would be an acceptable way to accomodate your
resource request.  This usually corresponds one-to-one with instance types,
but this is not required.

\subsubsection{Expert Mode}

The man page (in section \ref{man-condor-annex}) lists the ``expert
mode'' options.

Four of the ``expert mode'' options set the URLs used to access AWS services,
not including the CloudFormation URL needed by the \Opt{-setup} flag.  You
may change the CloudFormation URL by changing the HTCondor configuration
macro \Macro{ANNEX\_DEFAULT\_CF\_URL}, or by supplying the URL as the third
parameter after the \Opt{-setup} flag.  If you change any of the URLs,
you may need to change all of the URLs -- Lambda functions and CloudWatch
events in one region don't work with instances in another region.

You may also temporarily specify a different AWS account by using the
access (\Opt{-aws-access-key-file}) and
secret key (\Opt{-aws-secret-key-file}) options.  Regular users may have
an accounting reason to do this.

The options labeled ``developers only'' control implementation details and
may change without warning; they are probably best left unused unless you're
a developer.

%% Developers should but don't have an option to set the connectivity-checking
%% Lambda function's name (or ARN).

\section{Using \Condor{annex} for the First Time}
\label{sec:clouds-annex-first-time}

This guide assumes that you already have an AWS account, as well as a log-in
account on a Linux machine with a public address and a system administrator
who's willing to open a port for you.  All the terminal commands (shown in a
box without a title) and file edits (shown in a box with an
\emph{emphasized title}) take place on the Linux machine.  You can perform the
web-based steps from wherever is convenient, although it will save you some
copying if you run the browser on the Linux machine.

Before using \Condor{annex} for the first time, you'll have to do three things:

\begin{enumerate}
\item install a personal HTCondor
\item prepare your AWS account
\item configure \Condor{annex}
\end{enumerate}

Instructions for each follow.

\subsection{Install a Personal HTCondor}

We recommend that you install a personal HTCondor to make use of \Condor{annex};
it's simpler to configure that way.  These instructions assume version 8.7.8
of HTCondor, but should work the 8.8.x series as well; change `8.7.8' in
the instructions wherever it appears.

These instructions assume that it's OK to create a directory named
\texttt{condor-8.7.8} in your home directory; adjust them accordingly if you
want to install HTCondor somewhere else.

Start by downloading (from
\url{https://research.cs.wisc.edu/htcondor/downloads/}) the 8.7.8 release from
the ``tarballs'' section that matches your Linux version.  (If you don't know
your Linux version, ask your system administrator.)  These instructions assume
that the file you downloaded is located in your home directory on the Linux
machine, so copy it there if necessary.

Then do the following; note that in this box, like other terminal boxes,
the commands you type are preceded by by `\$' to distinguish them from any
expected output, so don't copy that part of each of the following lines.
(Lines which end in a `\textbackslash' continue on the following line; be
sure to copy both lines.  Don't copy the `\textbackslash' itself.)

\terminal{%
\$ mkdir \textasciitilde{}/condor-8.7.8; cd \textasciitilde{}/condor-8.7.8; mkdir local \\
\$ tar -z -x -f \textasciitilde{}/condor-8.7.8-*-stripped.tar.gz \\
\$ ./condor-8.7.8-*-stripped/condor\_install --local-dir `pwd`/local \textbackslash \\
\hphantom{\$ ./}--make-personal-condor \\
\$ .\ ./condor.sh \\
\$ condor\_master
}

\subsubsection{Testing}
\label{sec:clouds-user-guide-testing}

Give HTCondor a few seconds to spin up and the try a few commands to make sure
the basics are working.  Your output will vary depending on the time of day,
the name of your Linux machine, and its core count, but it should generally be
pretty similar to the following.

% LaTeX typsets lines with a leading hyphen without the usual padding, which
% causes that line's characters not to align vertically.  We'll just elide
% the dash at the beginning of the second line so it looks right.
%
% Note that we've got the trailing % on all of these so we can have the
% first line of the [file|term]inal start in the first column of this file.
{\obeyspaces\terminal{%
\$ condor\_q \\
\hphantom{xx} Schedd: submit-3.batlab.org : <127.0.0.1:12815?... @ 02/03/17 13:57:35 \\
OWNER    BATCH\_NAME         SUBMITTED   DONE   RUN    IDLE  TOTAL JOB\_IDS \\
\\
0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended \\
\$ condor\_status -any \\
MyType             TargetType         Name \\
\\
Negotiator         None               NEGOTIATOR \\
Collector          None               Personal Condor at 127.0.0.1@submit-3 \\
Machine            Job                slot1@submit-3.batlab.org \\
Machine            Job                slot2@submit-3.batlab.org \\
Machine            Job                slot3@submit-3.batlab.org \\
Machine            Job                slot4@submit-3.batlab.org \\
Machine            Job                slot5@submit-3.batlab.org \\
Machine            Job                slot6@submit-3.batlab.org \\
Machine            Job                slot7@submit-3.batlab.org \\
Machine            Job                slot8@submit-3.batlab.org \\
Scheduler          None               submit-3.batlab.org \\
DaemonMaster       None               submit-3.batlab.org \\
Accounting         none               <none>
}}

You should also try to submit a job; create the following file.  (We'll
refer to the contents of the box by the \emph{emphasized filename} in later
terminals and/or files.)

\fileinal{\textasciitilde{}/condor-annex/sleep.submit}{%
executable = /bin/sleep \\
arguments = 600 \\
queue
}

and submit it:

\terminal{%
\$ condor\_submit \textasciitilde{}/condor-annex/sleep.submit \\
Submitting job(s). \\
1 job(s) submitted to cluster 1. \\
\$ condor\_reschedule
}

After a little while:

{\obeyspaces\terminal{%
\$ condor\_q \\
\\
\\
\hphantom{xx} Schedd: submit-3.batlab.org : <127.0.0.1:12815?... @ 02/03/17 13:57:35 \\
OWNER    BATCH\_NAME         SUBMITTED   DONE   RUN    IDLE  TOTAL JOB\_IDS \\
tlmiller CMD: /bin/sleep   2/3  13:56      \_      1      \_      1 3.0 \\
\\
1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended
}}

\subsubsection{Configure Public Interface}

The default personal HTCondor uses the ``loopback'' interface, which basically
just means it won't talk to anyone other than itself.  For \Condor{annex} to
work, your personal HTCondor needs to use the Linux machine's public interface.
In most cases, that's as simple as adding the following lines:

\fileinal{\textasciitilde{}/condor-8.7.8/local/condor\_config.local}{%
NETWORK\_INTERFACE = * \\
CONDOR\_HOST = \$(FULL\_HOSTNAME)
}

Restart HTCondor to force the changes to take effect:

\terminal{%
\$ condor\_restart \\
Sent "Restart" command to local master
}

To verify that this change worked, repeat the steps under section
\ref{sec:clouds-user-guide-testing}.  Then proceed onto the next section.

\subsubsection{Configure a Pool Password}

In this section, you'll configure your personal HTCondor to use a pool
password.  This is a simple but effective method of securing HTCondor's
communications to AWS.

Add the following lines:

\fileinal{\textasciitilde{}/condor-8.7.8/local/condor\_config.local}{%
SEC\_PASSWORD\_FILE = \$(LOCAL\_DIR)/condor\_pool\_password \\
SEC\_DAEMON\_INTEGRITY = REQUIRED \\
SEC\_DAEMON\_AUTHENTICATION = REQUIRED \\
SEC\_DAEMON\_AUTHENTICATION\_METHODS = PASSWORD \\
SEC\_NEGOTIATOR\_INTEGRITY = REQUIRED \\
SEC\_NEGOTIATOR\_AUTHENTICATION = REQUIRED \\
SEC\_NEGOTIATOR\_AUTHENTICATION\_METHODS = PASSWORD \\
SEC\_CLIENT\_AUTHENTICATION\_METHODS = FS, PASSWORD \\
ALLOW\_DAEMON = condor\_pool@*
}

You also need to run the following command, which prompts you to enter a
password:

\terminal{%
\$ condor\_store\_cred -c add -f `condor\_config\_val SEC\_PASSWORD\_FILE` \\
Enter password:
}

Enter a password.

\subsubsection{Tell HTCondor about the Open Port}

By default, HTCondor will use port 9618.  If the Linux machine doesn't already
have HTCondor installed, and the admin is willing to open that port, then you
don't have to do anything.  Otherwise, you'll need to add a line like the
following, replacing `9618' with whatever port the administrator opened for you.

\fileinal{\textasciitilde{}/condor-8.7.8/local/condor\_config.local}{%
COLLECTOR\_HOST = \$(FULL\_HOSTNAME):9618
}

\subsubsection{Activate the New Configuration}

Force HTCondor to read the new configuration by restarting it:

\terminal{%
\$ condor\_restart
}

\subsection{Prepare your AWS account}

Since v8.7.1, the \Condor{annex} tool has included a \texttt{-setup} command which will
prepare your AWS account.

\subsubsection{Obtaining an Access Key}

In order to use AWS, \Condor{annex} needs a pair of security tokens (like a
user name and password).  Like a user name, the ``access key'' is (more or
less) public information; the corresponding ``secret key'' is like a password
and must be kept a secret.  To help keep both halves secret,
\Condor{annex} (and HTCondor) are never told these keys directly; instead, you
tell HTCondor which file to look in to find each one.

Create those two files now; we'll tell you how to fill them in shortly.  By
convention, these files exist in your \texttt{\textasciitilde{}/.condor}
directory, which is where the \texttt{-setup} command will store the rest of
the data it needs.

\terminal{%
\$ mkdir \textasciitilde{}/.condor \\
\$ cd \textasciitilde{}/.condor \\
\$ touch publicKeyFile privateKeyFile \\
\$ chmod 600 publicKeyFile privateKeyFile
}

The last command ensures that only you can read or write to those files.

To donwload a new pair of security tokens for \Condor{annex} to use, go to
the IAM console at the following URL; log in if you need to:\\
\\
\url{https://console.aws.amazon.com/iam/home?region=us-east-1#/users}\\
\\
The following instructions assume you are logged in as a user
with the privilege to create new users.  (The `root' user for any account has
this privilege; other accounts may as well.)

\begin{enumerate}
\item Click the ``Add User'' button.
\item Enter name in the \textbf{User name} box; ``annex-user'' is a fine choice.
\item Click the check box labelled ``Programmatic access''.
\item Click the button labelled ``Next: Permissions''.
\item Select ``Attach existing policies directly''.
\item Type ``AdministratorAccess'' in the box labelled ``Filter''.
\item Click the check box on the single line that will appear below (labelled ``AdministratorAccess'').
\item Click the ``Next: review'' button (you may need to scroll down).
\item Click the ``Create user'' button.
\item From the line labelled ``annex-user'', copy the value in the column labelled ``Access key ID'' to the file \texttt{publicKeyFile}.
\item On the line labelled ``annex-user'', click the ``Show'' link in the column labelled ``Secret access key''; copy the revealed value to the file \texttt{privateKeyFile}.
\item Hit the ``Close'' button.
\end{enumerate}

The `annex-user' now has full privileges to your account.

\subsection{Configure \Condor{annex}}

The following command will setup your AWS account.  It will create a number
of persistent components, none of which will cost you anything to keep around.
These components can take quite some time to create; \Condor{annex} checks
each for completion every ten seconds and prints an additional dot (past the
first three) when it does so, to let you know that everything's still working.

\terminal{%
\$ condor\_annex -setup \\
Creating configuration bucket (this takes less than a minute)....... complete. \\
Creating Lambda functions (this takes about a minute)........ complete. \\
Creating instance profile (this takes about two minutes)................... complete. \\
Creating security group (this takes less than a minute)..... complete. \\
Setup successful.
}

\subsubsection{Checking the Setup}

You can verify at this point (or any later time) that the setup procedure
completed successfully by running the following command.

\terminal{%
\$ condor\_annex -check-setup \\
Checking for configuration bucket... OK. \\
Checking for Lambda functions... OK. \\
Checking for instance profile... OK. \\
Checking for security group... OK.
}

You're ready to run \Condor{annex}!

\subsubsection{Undoing the Setup Command}

There is not as yet a way to undo the setup command automatically, but it
won't cost you anything extra to leave your account setup for \Condor{annex}
indefinitely.  If, however, you want to be tidy, you may delete the components
setup created by going to the CloudFormation console at the following URL
and deleting the entries whose names begin with `HTCondorAnnex-':\\
\\
\url{https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks?filter=active}\\
\\
The setup procedure also creates an SSH key pair which may be useful
for debugging; the private key was stored in
\texttt{\textasciitilde{}/.condor/HTCondorAnnex-KeyPair.pem}.  To remove the
corresponding public key from your AWS account, go to the key pair console
at the following URL and delete the `HTCondorAnnex-KeyPair' key:\\
\\
\url{https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#KeyPairs:sort=keyName}
