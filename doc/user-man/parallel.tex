%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\label{sec:Parallel}Parallel Applications (Including MPI Applications)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\index{parallel universe|(}
\index{MPI application}

HTCondor's parallel universe supports jobs that span multiple machines,
where the multiple processes within a job must be running concurrently
on the various matched and claimed machines.
The parallel universe provides machine scheduling,
but does not enforce a particular programming paradigm for the
underlying applications.
Thus, parallel universe jobs may run under various MPI implementations
as well as under other programming environments. 

The parallel universe supersedes the mpi universe.
The mpi universe eventually will be removed from HTCondor.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\label{sec:parallel-model}How Parallel Jobs Run}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Parallel universe jobs are submitted from the machine running 
the dedicated scheduler.
The dedicated scheduler matches and claims a fixed number machines (slots)
for the parallel universe job,
and when the correct number of machines are claimed,
the parallel job is started.

Each invocation of \Condor{submit} assigns a single \Attr{ClusterId}
for what is considered the single parallel job submitted.
The \SubmitCmd{machine\_count} submit command identifies how 
many machines (slots) are to be allocated.
Each instance of the \SubmitCmd{queue} submit command acquires
and claims the number of slots specified by \SubmitCmd{machine\_count}.
Each of these slots shares a common job ClassAd and
will have the same \Attr{ProcId} job ClassAd attribute value.

Once the correct number of machines are claimed, 
the \SubmitCmd{executable} is started at more or less the same
time on all machines.
If desired, a monotonically increasing integer value that starts
at 0 may be provided to each of these machines.
The macro \MacroUNI{Node} is similar to the MPI \Term{rank}
construct. This macro may be used within the submit description
file in either the \SubmitCmd{arguments} or \SubmitCmd{environment}
command.
Thus, as the executable runs, it may discover its own \MacroUNI{Node}
value.

Node 0 has special meaning and consequences for the parallel job.
The completion of a parallel job is implied and taken to be
when the Node 0 executable exits.  All other nodes that are
part of the parallel job and that have not yet exited on their 
own are killed.
This default behavior may be altered by placing the line
\begin{verbatim}
+ParallelShutdownPolicy = "WAIT_FOR_ALL"
\end{verbatim}
in the submit description file.
It causes HTCondor to wait until every node in the parallel 
job has completed to consider the job finished. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\label{sec:parallel-setup}Parallel Jobs and the Dedicated Scheduler}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To run parallel universe jobs, HTCondor must be configured such that 
\index{scheduling!dedicated}
machines running parallel jobs are \Term{dedicated}.  
Note that dedicated has a very specific meaning in HTCondor:
while dedicated machines can run serial jobs, they prefer to run
parallel jobs, and dedicated machines never preempt a parallel job 
once it starts running.

A machine becomes a dedicated machine when an administrator configures
it to accept parallel jobs from one specific dedicated scheduler.  
Note the difference between parallel and serial jobs.
While any scheduler in a pool can send serial jobs to any machine,
only the designated dedicated scheduler may send parallel universe
jobs to a dedicated machine.
Dedicated machines must be specially configured.  
See section~\ref{sec:Config-Dedicated-Jobs} for a description
of the necessary configuration, as well as examples.
Usually, a single dedicated scheduler is configured for a pool
which can run parallel universe jobs, and this \Condor{schedd} daemon
becomes the single machine from which parallel universe
jobs are submitted.

The following command line will list the execute machines 
in the local pool which have been configured to use a dedicated
scheduler, also printing the name of that dedicated scheduler.
In order to run parallel jobs, this name will be defined to be
the string \AdStr{DedicatedScheduler@}, prepended to the name of the
scheduler host.

\footnotesize
\begin{verbatim}
  condor_status -const '!isUndefined(DedicatedScheduler)' \
	-format "%s\t" Machine -format "%s\n" DedicatedScheduler

execute1.example.com	DedicatedScheduler@submit.example.com
execute2.example.com	DedicatedScheduler@submit.example.com

\end{verbatim}
\normalsize

If this command emits no lines of output, then then pool is
not correctly configured to run parallel jobs.
Make sure that the name of the scheduler is correct. 
The string after the \Expr{@} sign should match the name of the 
\Condor{schedd} daemon, as returned by the command

\begin{verbatim}
  condor_status -schedd
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\label{sec:parallel-submit}Submission Examples}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{description}
\item[Simplest Example]
\end{description}

Here is a submit description file for a parallel universe
job example that is as simple as possible:

\begin{verbatim}
#############################################
##   submit description file for a parallel universe job
#############################################
universe = parallel
executable = /bin/sleep
arguments = 30
machine_count = 8
log = log
should_transfer_files = If_NEEDED
when_to_transfer_output = ON_EXIT
queue 
\end{verbatim}

This job specifies the \SubmitCmd{universe} as \SubmitCmd{parallel}, letting
HTCondor know that dedicated resources are required.  The
\SubmitCmd{machine\_count} command identifies the number of machines
required by the job. 

When submitted, the dedicated scheduler allocates eight
machines with the same architecture and operating system as the submit
machine.  It waits until all eight machines are available before
starting the job.  When all the machines are ready, it invokes the
\Prog{/bin/sleep} command, with a command line argument of 30
on all eight machines more or less simultaneously.  As with serial
jobs, important event in the life cycle of the job are written
to the user log noted in the log command.

\begin{description}
\item[Example with Operating System Requirements]
\end{description}

Parallel universe jobs have the familiar Requirements expression
that serial jobs do, and setting the requirements expression will
restrict the machines that match your job.

For example, if your pool consists of Linux machines installed with the RedHat and Ubuntu operating systems, and you'd like to run on only the RedHat machines, you can use
the following requirements expression:

\begin{verbatim}
#############################################
##   submit description file for a parallel program targeting RedHat machines
#############################################
universe = parallel
executable = /bin/sleep
arguments = 30
machine_count = 8
log = log
output = output
error  = error
notification = never
should_transfer_files = always
when_to_transfer_output = on_exit
requirements = (OpSysName == "RedHat")
queue 

\end{verbatim}

In addition, you may narrow down your machine selection to the version you'd like to run on using the OpSysAndVer attribute.

\begin{verbatim}
#############################################
##   submit description file for a parallel program targeting RedHat 6 machines
#############################################
universe = parallel
executable = /bin/sleep
arguments = 30
machine_count = 8
log = log
output = output
error  = error
notification = never
should_transfer_files = always
when_to_transfer_output = on_exit
requirements = (OpSysAndVer == "RedHat6")
queue
\end{verbatim}

\begin{description}
\item[Using the \MacroUNI{Node} Macro]
\end{description}

A more realistic example of a parallel job utilizes other features.

\begin{verbatim}
######################################
## Parallel example submit description file
######################################
universe = parallel
executable = /bin/cat
log = logfile
input = infile.$(NODE)
output = outfile.$(NODE)
error = errfile.$(NODE)
machine_count = 4
notification = never
should_transfer_files = always
when_to_transfer_output = on_exit
queue
\end{verbatim}

The specification of the \SubmitCmd{input}, \SubmitCmd{output},
and \SubmitCmd{error} files utilize the predefined macro 
\MacroUNI{NODE}.
\index{macro!predefined}
See the \Condor{submit}
manual page on page~\pageref{man-condor-submit} for further
description of predefined macros.
The \MacroU{NODE} macro is given a
unique integer value, starting at zero as processes are assigned to machines.
This is similar to the MPI notion of "rank".
The \MacroUNI{NODE} value is fixed for the entire length of the job.
It can therefore be used to identify individual aspects of the computation.
In this example, it is used to utilize and assign unique names to
input and output files.

\begin{description}
\item[Differing Requirements for the Machines]
\end{description}

Usually, all the machines running a parallel job will have the
same requirements.  Sometimes, though, one machine will need
to be different than the rest.  
The different subsets of machines executing a parallel universe job
may specify different machine requirements.  A common example requires that the
head node execute on a specific machine.  It may be also useful for debugging purposes.

To demonstrate how to have parallel jobs with different requirements,
consider the following example.

\begin{verbatim}
######################################
## Example submit description file
## with multiple procs
######################################
universe = parallel
executable = example
machine_count = 1
requirements = ( machine == "machine1")
queue

requirements = ( machine =!= "machine1")
machine_count = 3
queue
\end{verbatim}

The dedicated scheduler allocates four machines.
All four executing jobs have the same value for \MacroUNI{Cluster}
macro.
The \MacroUNI{Process} macro takes on two values;
the value 0 will be assigned for the single executable
that must be executed on machine1, and
the value 1 will be assigned for the other three 
that must be executed anywhere but on machine1.

Carefully consider the ordering and nature of multiple
sets of requirements in the same submit description file.
The scheduler matches jobs to machines based on the ordering
within the submit description file.
Mutually exclusive requirements eliminate the dependence on
ordering within the submit description file.
Without mutually exclusive requirements,
the scheduler may be unable to schedule the job.
The ordering within the submit description file may preclude
the scheduler considering the specific allocation that
could satisfy the requirements.

The way to create multiple processes is to place a number after the \SubmitCmd{queue}
command, such as \SubmitCmd{queue 8} to create eight different processes with the 
same cluster. After a particular \SubmitCmd{queue} command, the user can also change 
parameters previously specified in the submit file or add parameters not 
previously specified. For example, suppose that the head node of a parallel job 
needs additional RAM compared to the rest of the nodes. The user can write

\begin{verbatim}
request_memory = 5G
queue
request_memory = 1G
queue 31
\end{verbatim}

If a user wishes to submit multiple independent parallel universe jobs, the 
user cannot simply append a number to the \SubmitCmd{queue} command. HTCondor will 
assume that each job is not independent -- as by definition, a parallel job is a 
job is at least setup to handle multiple (sub)processes. Instead, the user will
need to run \Condor{submit} multiple times.

\label{sec:parallel-mpi-submit}
\begin{description}
\item[MPI Applications]
\end{description}

\index{parallel universe!running MPI applications}
\index{MPI application}

MPI applications consist of a single executable that is invoked in order to
execute in parallel on one or more machines. 
HTCondor's parallel universe provides the environment within
which this executable is executed in parallel.
However, the various implementations of MPI
(for example, Open MPI or MPICH) require further framework items within
a system-wide environment.
HTCondor supports this necessary framework through 
user visible and modifiable scripts.
An MPI implementation-dependent script becomes the HTCondor job.
The script sets up the extra, necessary framework,
and then invokes the MPI application's executable.

HTCondor provides these scripts in the
\File{\MacroUNI{RELEASE\_DIR}/etc/examples}
directory.
The script for the Open MPI implementation is \File{openmpiscript}.
The script for the MPICH implementation is \File{mp1script}.
One restriction of these scripts is that they rely on running
the ssh command from one node to another.  The ssh daemon 
on Unix does not allow connections if the shell named in
the password file is not in the set of approved shells
listed in the /etc/shells file.

Therefore, an HTCondor submit description file for these
implementations would appear similar to:

\begin{verbatim}
######################################
## Example submit description file
## for MPICH 1 MPI
## works with MPICH 1.2.4, 1.2.5 and 1.2.6
######################################
universe = parallel
executable = mp1script
arguments = my_mpich_linked_executable arg1 arg2
machine_count = 4
should_transfer_files = yes
when_to_transfer_output = on_exit
transfer_input_files = my_mpich_linked_executable
queue
\end{verbatim}

or

\begin{verbatim}
######################################
## Example submit description file
## for Open MPI
######################################
universe = parallel
executable = openmpiscript
arguments = my_openmpi_linked_executable arg1 arg2
machine_count = 4
should_transfer_files = yes
when_to_transfer_output = on_exit
transfer_input_files = my_openmpi_linked_executable
queue
\end{verbatim}

The \SubmitCmd{executable} is the MPI implementation-dependent script.
The first argument to the script is the MPI application's 
executable.
Further arguments to the script are the MPI application's arguments.
HTCondor must transfer this executable;
do this with the \SubmitCmd{transfer\_input\_files} command.

For other implementations of MPI,
copy and modify one of the given scripts.
Most MPI implementations require two system-wide prerequisites.
The first prerequisite is the ability to run a command
on a remote machine without being prompted for a password.
\Prog{ssh} is commonly used, but other
commands may be used.
The second prerequisite is an ASCII file containing the
list of machines that may utilize \Prog{ssh}.
These common prerequisites are implemented in a further script
called \File{sshd.sh}.
\File{sshd.sh} generates ssh keys 
(to enable password-less remote execution),
and starts an \Prog{sshd} daemon.
The machine name and MPI rank are given to the submit machine.

%So, to run MPI application in the parallel universe, we run a script
%on each node we submit to.  This script generates ssh keys, to enable
%password-less remote execution, start an sshd daemon, and send the
%names and rank (node number) back to the submit directory.  Thus, for
%each HTCondor job submitted, the scripts set up an ad-hoc MPI
%environment, which is torn down at the end of the job run.  This ssh
%script is a common requirement for running MPI jobs, so we have
%factored it out into a common script, which is called from each of the
%MPI-specific scripts.  After the ssh script has been started, the
%MPI-specific script runs, starts the rest of the MPI job by looking at
%its arguments, and waits for the MPI job to finish.  HTCondor provides
%the ssh script, and example MPI scripts for both LAM and MPICH.  The
%former is named ``lamscript'', and the latter ``mp1script''.  The
%first argument to each script is the name of the real MPI executable,
%and any subsequent arguments are arguments to that executable.  Other
%implementations should be easy to add, by modifying the given
%examples.  Note that because the actual MPI executable (i.e. the
%output of mpicc) is not the named executable in the submit script, it
%must be accessible either via a network file system, or by condor file
%transfer.

The \Prog{sshd.sh} script requires the definition of
two HTCondor configuration variables.
Configuration variable \Macro{CONDOR\_SSHD} is an absolute path to
an implementation of \Prog{sshd}.
\Prog{sshd.sh} has been tested with \Prog{openssh} version 3.9,
but should work with more recent versions.
Configuration variable \Macro{CONDOR\_SSH\_KEYGEN} points
to the corresponding \Prog{ssh-keygen} executable.

Scripts \Prog{openmpiscript} and \Prog{mp1script}
each have their own idiosyncrasies.
In \Prog{mp1script}, the \Env{PATH} to the MPICH installation must be set.
The shell variable MPDIR indicates its proper value.
This directory contains the MPICH \Prog{mpirun} executable; Open MPI also
uses MPDIR.
%For OpenMPI, there is a similar path setting, but it is called \Env{LAMDIR}
%in the \Prog{openmpiscript} script.  
%In addition, this path must be part of the
%path set in the user's \File{.cshrc} script.
%As of this writing, the LAM implementation does not work
%if the user's login shell is the Bourne or compatible shell.

\index{parallel universe|)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\label{sec:parallel-mpi-submit-single}MPI Applications Within HTCondor's Vanilla Universe}
%Within HTCondor's Parallel Universe}
%Running MPI Jobs On A Single Machine}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

With partitionable slots, a user may wish to run a parallel job written with
MPI but run only on one machine. Parallel universe is designed to help 
coordinate a job on multiple machines and thus is not the correct route for
this kind of job. Instead, the vanilla universe should be used with partitionable
slots and the \SubmitCmd{request\_cpus} option should be used in the HTCondor submit
file. To have a machine use 8 CPUs, simple put in your submit description file:

\begin{verbatim}
request_cpus = 8
\end{verbatim}

A frequently asked question is: ``How do I make sure my MPI job can run 
on any machine that it lands on?'' There are two answers:
\begin{enumerate}
\item Statically build an MPI library and statically compile your MPI code.
\item Use CDE to create a directory tree that contains all of the libraries 
needed to execute your code.
\end{enumerate}

For Linux machines, we have found that building static MPI libraries can be 
difficult and that 
using CDE and some shell scripts provides the requested results. CDE can be
found at \URL{http://www.pgbovine.net/cde.html}.

Ff MPI is installed dynamically on all machines/nodes on which the program 
could run or if the code was built static libraries and a static version of 
\File{mpirun} is available, the following
submit description file works well.
\begin{verbatim}
################################################################################
##   Submit Description File for a Parallel Program in Vanilla Universe
################################################################################
universe = vanilla
executable = /path/to/mpirun
request_cpus = 2
arguments = -np 2 my_mpi_linked_executable arg1 arg2 arge
should_transfer_files = yes
when_to_transfer_output = on_exit
transfer_input_files = my_mpi_linked_executable

queue
\end{verbatim}

If MPI is not installed on all potential machines and CDE is to be used,
then CDE needs to be run first to create the directory tree. On the host machine
which has the original program, the command

\begin{verbatim}
prompt-> cde mpirun -n 2 my_mpi_linked_executable
\end{verbatim}

will create a directory tree that will contain all libraries needed for the 
program. By creating a tarball of this directory, the user can easily package up
the executable itself, any files needed for the executable, and all necessary
libraries. In the following example, we assume that the user has created a 
tarball called \File{cde\_my\_mpi\_linked\_executable.tar} which contains the 
directory tree created by CDE.

%this submit description file works well.
\begin{verbatim}
#############################################
##   submit description file for a parallel program
#############################################
universe = vanilla
executable = cde_script.sh
request_cpus = 2
should_transfer_files = yes
when_to_transfer_output = on_exit
transfer_input_files = cde_my_mpi_linked_executable.tar
transfer_output_files = cde-package/cde-root/path/to/original/directory
queue
\end{verbatim}

%We assume that the user has created a tarball called 
%\File{cde\_my\_mpi\_linked\_executable.tar} which contains the directory tree
%created by CDE.
The contents of \Prog{cde\_script.sh} are:
\begin{verbatim}
#!/bin/sh
# Untar the CDE package
tar xpf cde_my_mpi_linked_executable.tar
# cd to the subdirectory where I need to run
cd cde-package/cde-root/path/to/original/directory
# Run my command
./mpirun.cde -n 2 ./my_mpi_linked_executable
# Normally, HTCondor will transfer the contents of this directory.
# However, we don't want the .cde command and the executable transferred back
# To prevent this from happening, we manually remove both files.
rm -f mpirun.cde
rm -f my_mpi_linked_executable
\end{verbatim}

Any additional input files needed for the executable not already in the tarball
should be included in the list in \SubmitCmd{transfer\_input\_files} command. 
The corresponding script should then also be updated to move those files into
the directory that where the executable will be run.


