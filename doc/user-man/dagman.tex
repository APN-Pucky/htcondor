\section{Interjob Dependencies: DAGMan Meta-Scheduler}
\label{sec:DAGMan}

The Directed Acyclic Graph Manager (DAGMan) is a meta-scheduler for Condor
jobs.  DAGMan is responsible for submitting batch jobs in a predefined order
and processing the results. A configuration file is defined prior to execution
of DAGMan in which the jobs, their \textit{CondorConfigFile}, and job
dependencies are declared.

The importance of such a tool lies in the fact that the user is able to define
the execution order of a number of Condor Jobs. Just as Condor schedules
condor jobs, DAGMan schedules a system of jobs. In essence, it defines a
problem. Solving a problem may require multiple condor jobs that need data
from each other. This is best represented using a Directed Acyclic Graph
(DAG), which represents the flow of control from one node to another (i.e.,
from one condor job to another) through arrows.

From the point of view of the user, the scheduler is initialized with the
order of execution of jobs, and then started. DAGMan is responsible for all
scheduling, recovery and reporting activities of the submitted system of jobs.

The following sections explain the use of DAGMan in full detail.  However, if
the user only wants the bare essentials, please read
section~\ref{dagman:essentials} to get started more quickly.

\subsection{DAG Input File}

For Unix users, a useful analogy might be to think of the DAGMan input file as
a makefile, and DAGMan itself as the make executable.  However, DAGMan differs
from make.  Instead of looking at file modification timestamps, DAGMan reads
the Condor log file generated by each Condor job to find out which jobs are
unsubmitted, submitted, or complete.  DAGMan also makes a guarantee that a DAG
is recoverable, even if the machine running DAGMan goes down during execution.

\subsubsection{Description}
\label{dagman:dagdesc}

Job dependencies are defined prior to execution of the DAGMan program, using a
DAG input file.  An example input configuration file name is \File{diamond.dag}.
The input file is read completely, and the DAG data structure is constructed
in memory before the first job is submitted.  With the exception of the
\textit{CondorCommandFile} (see below), the input file is case insensitive.

Throughout the input file, comments can be placed.  Legal comments exist on a
single line which immediately starts with a `\texttt{\#}' character, followed
by any characters up to the newline `\texttt{$\backslash$n}'.

It is interesting to note that the DAGMan input file does not contain any
specifics about the individual jobs. Each condor job by itself is handled as
if DAGMan was not present (this includes compiling and linking of the
job). The executable and the input/output parameters for each job are
contained in the CondorCommandFile.  The DAG file merely describes the
relationship between the different condor jobs using the semantics just
described.

\begin{description}

\item[Signature]

The first line of a DAG input file is the signature, which precisely
identifies which DAG file format follows.  As of this writing, only one DAG
format exists, and thus only one signature is possible.

\begin{verbatim}
  ### DAGMan 6.1.0
\end{verbatim}

This line must appear as it is written here, character for character.
Anything different will be rejected by DAGMan.  Having a precise signature tag
will enable future versions of DAGMan to remain backward compatible with older
DAG input file formats.

\item[Job Section]

The Job Section of the input DAG file declares all the jobs that will appear
in the DAG.  Each job is described by a single line called a Job Entry.  The
following syntax is used:

\begin{verbatim}
	JOB <JobName> <CondorCommandFile>
\end{verbatim}

The \texttt{JOB} keyword (shown here in upper case only for clarity) declares
this line will map a \textit{JobName} to a Condor Command File.  The
\textit{JobName} is used by DAGMan to uniquely identify jobs throughout the
input file and to name them in output messages.  The
\textit{CondorCommandFile} is the input file used by \Condor{submit} to run
the individual condor job.  Because the Unix file system is case sensitive,
the case of the \textit{CondorCommandFile} is preserved.

The JobName can be any string that contains no white space.  The JobName is
not case sensitive, so ``JobA'' is equivalent to ``joba''.  An example
\textit{CondorCommandFile} name is \File{a.condor}.  Some important
restrictions are placed on the contents of the \textit{CondorCommandFile},
which will be discussed later.

The user can also have the option of declaring a job as being already
completed in the DAG input file. This may be useful in situations where the
user wishes to verify results, but does not need the entire job dependency
graph to be executed. This is done by adding the word "DONE" to the end of the
Job declaration line.

\begin{verbatim}
	JOB <JobName> <CondorCommandFile> DONE
\end{verbatim}

\item[Dependency Section]

The dependency section of the DAG input file follows the Job Section and
describes the dependencies between the jobs listed in the Job Section.  The
notion of a ``parent'' and ``child'' job is introduced here.  A parent job
produces output which is required by one or more child jobs.  None of the
children can run until the parent successfully terminates.  A child job is one
whose input is taken from one or more parent jobs.  The child job cannot run
until all of its parents have successfully terminated.

A single line in the input file can specify the dependencies from one or more
parents to one or more children.

\begin{verbatim}
	PARENT <ParentJobName>* CHILD <ChildJobName>*
\end{verbatim}

The \texttt{PARENT} keyword is followed by one or more
\textit{ParentJobName}s.  Those are followed by the \texttt{CHILD} keyword,
which is followed by one or more \textit{ChildJobName}s.  Each child job
depends on each and every parent job on this line.  So the line
``\texttt{PARENT p1 p2 CHILD c1 c2}'' would produce four dependencies.

\end{description}

\subsubsection{Example}

The following \File{diamond.dag} DAG input file shown below is illustrated in
Figure~\ref{fig:dagman-diamond}.

\begin{verbatim}
  ### DAGMan 6.1.0
  # Filename: diamond.dag
  #
  Job  A  A.condor 
  Job  B  B.condor 
  Job  C  C.condor	
  Job  D  D.condor
  PARENT A CHILD B C
  PARENT B C CHILD D
\end{verbatim}

\begin{figure}[hbt]
\centering
\includegraphics{user-man/dagman-diamond.eps}
\caption{\label{fig:dagman-diamond}Diamond DAG}
\end{figure}

With \File{diamond.dag}, job A must execute first, because all other jobs
directly or indirectly depend on it.  After job A successfully completes, both
job B and C are eligible to run.  In fact, they will be submitted at the same
time and hopefully Condor will find two remote hosts that can run them in
parallel.  Since job D depends on both B and C, it must wait for both to
complete successfully before it can be submitted.

\subsection{Execution}

\subsubsection{Preparing Jobs}
\label{dagman:prepjob}

Each individual job in a DAG is free to be a unique executable, with a unique
\textit{CondorCommandFile}.  The DAG can contain a mixture of standard and
vanilla jobs, or even other meta-scheduler jobs, like DAGMan.  On the other
hand, the jobs in the DAG could all use the same executable, or even the same
\textit{CondorCommandFile}.  Anything between both extremes is possible.
However, two limits are imposed.

First, each \textit{CondorCommandFile} must submit a cluster of size one.
There cannot be multiple \texttt{queue} lines.  The reasoning is long winded,
so a brief summary will be attempted.  If multi-job clusters were allowed,
DAGMan would have to parse the \textit{CondorCommandFile} to find out how many
jobs belong to that cluster.  Otherwise, DAGMan would not know for sure if a
cluster had terminated based on seeing the event from one job of that
cluster.  This restriction may be lifted in future DAGMan version, depending
on the design and implementation issues.

Second, all \textit{CondorCommandFile}s of a DAG must specify the same log.
In order for DAGMan to follow the order of events correctly, all events from
all jobs in the DAG must be sent to the same log file.  This restriction will
be loosened in later versions (see section~\ref{dagman:version}).

For this example, we will write a single \textit{CondorCommandFile} to be used
by all three jobs in the DAG.  Thus, each job will run the same executable.
This example is very artificial, because normally separate jobs would need
output for their child jobs to go to unique output and error files.
Otherwise, the jobs would be clobbering each other's output.  However, since
we are sending output and error to \File{/dev/null}, sharing the
\textit{CondorCommandFile} is OK.

\begin{verbatim}
  # Filename: diamond_job.condor
  #
  executable   = /path/diamond.exe
  output       = /dev/null
  error        = /dev/null
  log          = diamond_condor.log
  universe     = vanilla
  notification = NEVER
  queue
\end{verbatim}

Note that notification is set to \texttt{NEVER}.  This is recommended if you
prefer not to have Condor send you e-mail for every job in a large DAG.

\subsubsection{Writing the DAG File}
\label{dagman:writedag}

The DAG file names the jobs, associates jobs with their
\textit{CondorCommandFile}, and declares job dependencies.  For our artificial
DIAMOND example, all three jobs will use the same diamond\_job.condor file
written earlier.  However, a more typical DAG file would have unique
\textit{CondorCommandFile} for every job.

\begin{verbatim}
  ### DAGMan 6.1.0
  # Filename: diamond.dag
  # DIAMOND DAG File for DAGMan
  #
  Job  A  diamond_job.condor
  Job  B  diamond_job.condor
  Job  C  diamond_job.condor
  Job  D  diamond_job.condor
  PARENT A CHILD B C
  PARENT B C CHILD D
\end{verbatim}

This DAG file will be the input file for the \Condor{dagman} program.

\subsubsection{Submitting the DAG to Condor}
\label{dagman:submitdag}

In order to guarantee recoverability, the DAGMan program itself is run as a
Condor job.  However, DAGMan is not submitted as a standard universe or
vanilla universe job.  Instead, it is run as a meta-scheduler.  Standard and
vanilla universe jobs are usually submitted to the local schedd, which
schedules them for execution on some remote machine in the pool that is idle.
A meta-scheduler is also submitted to the local schedd, but runs on the local
schedd.  The meta-scheduler then submits jobs, according to its design, to the
same local schedd, just as if the user submitted them manually.  In fact, the
local schedd does not know the difference between DAGMan submitting a job, and
the user who originally submitted DAGMan, and could have submitted the DAG
jobs manually.

A DAG is submitted using the \Condor{submit\_dag} script.  For example, to
submit the \File{diamond.dag} DAG to Condor, simply type
``\Condor{submit\_dag} \File{diamond.dag}''.  This script will generate the
\File{diamond.dag.condor} \textit{CondorCommandFile} for the DAG, and submit
it to Condor.

If the user prefers to edit the \File{diamond.dag.condor} file before it is
submitted to Condor (for example, to change the pre-chosen filenames), she can
issue ``\Condor{submit\_dag} -n \File{diamond.dag}'', which specifies that
\File{diamond.dag.condor} is generated, but not submitted to Condor.  To run
the DAG, issue the command \Condor{submit} diamond.dag.condor.

\subsection{Removal}

After submitting a DAG, the user may change her mind and wish to remove the
entire DAG, plus any jobs submitted by that DAG which happen to currently be
running.  DAG removal is easily accomplished by issuing a \Condor{rm} on the
DAGMan job itself.  The schedd sends a special signal to the meta-scheduler,
telling it to remove any of its condor jobs (using \Condor{rm}) that are
currently running.

However, if the machine is scheduled to go down, and the schedd receives a
shutdown command from the master, the schedd will send a running DAGMan job a
similar shutdown, which instructs DAGMan to clean up memory and exit.
However, in this case, DAGMan does not remove its submitted jobs, but rather
expects them to persistently exist in the Condor queue after restart.

The important thing to remember is that DAGMan will not explicitly run
\Condor{rm} on its jobs except as a result of the user running \Condor{rm} on
the DAGMan job.

\subsection{Recovery}

The Condor system offers the benefit of recoverability, in that if any host
crashes, Condor jobs that were running can be recovered, either by continuing
from the last checkpoint, or rerunning from scratch.  In any event, Condor
guarantees that once a job is successfully submitted, the Condor system will
not loose it.

DAGMan makes the same guarantee about the DAG as a whole.  If the machine
running DAGMan goes down or crashes, upon restart DAGMan will be restarted,
and the state of the DAG jobs will be recovered from the log file
(\File{diamond.dag.condor.log} from our example before).  DAGMan knows to
recover a DAG (as apposed to starting a new one) because it will detect the
existance of a lock file that was not removed from the last run.  If DAGMan
successfully finishes a DAG, the lock file is removed, so that the next run
will not go into recover mode.  The lock file is specified via command-line
argument to DAGMan in the \textit{CondorCommandFile}.  Refer to
section~\ref{dagman:submitdag}.

\subsection{Essentials}
\label{dagman:essentials}

This section is written for those users looking for the boiled down,
absolutely essential steps to successfully submit a DAG.

\begin{description}

\item[Prepare Jobs] Each job in the DAG must have its own
\textit{CondorCommandFile}.  Each \textit{CondorCommandFile} can only submit
one job.  Multi-job clusters (multiple \texttt{queue} lines) are not
supported.  The \texttt{log=} for all \textit{CondorCommandFile}s must point
to the same Condor log file, otherwise, DAGMan will not see all the Condor log
entries for every job in the DAG.  Refer to section~\ref{dagman:prepjob} for
details on how to prepare jobs.

\item[Write DAG File] Write the DAG file, so that JOB entries refer to the
\textit{CondorCommandFile}s you wrote in the previous step.  Refer to
section~\ref{dagman:writedag} to learn about writing a DAG file.

\item[Submit the DAG] Finally, you submit the DAG written in the previous step
using the \Condor{submit\_dag} script.  Refer to
section~\ref{dagman:submitdag}.

\end{description}


\subsection{Version Summary}
\label{dagman:version}

This section addresses the features and limitations that exist in the current
version of DAGMan, and how they may change in future versions.

This first public release of DAGMan was written and tested in the Condor 6.1.0
environment.  It is shipped separate from the main Condor system as a
contribution program.  As such, it is not as rigorously tested as the core
components of Condor.  A reasonable effort has been made to test large DAGS
(on the order of 5000 jobs) on Solaris x86 and Sparc.  However, the DAGMan is
not arrogant enough to claim itself bug free.  Users are encouraged to send
e-mail to \Email{condor-admin@cs.wisc.edu}.

The following feature summary compares the current version with possible
versions of DAGMan still to come.

\begin{description}
\item[Feature] : Command Socket
\item[Version 6.1.0] : Unsupported
\item[Future Versions] : A general purpose command socket will be used to
direct Dagman while it's running.  Commands like CANCEL\_JOB X or DELETE\_ALL
would be supported, as well as notification messages like JOB\_SUBMIT or
JOB\_TERMINATE, etc.  Eventually, a Java Gui would graphically represent the
Dag's state, and offer buttons and dials for graphic Dag manipulation.
\end{description}

\begin{description}
\item[Feature]: DAG removal
\item[Version 6.1.0]: Supported via \Condor{rm} of the DAG.
\item[Future Versions]: Supported by a command socket such as DELETE\_ALL
\end{description}

\begin{description}
\item[Feature]: Condor Log File
\item[Version 6.1.0]: All jobs in a DAG must specify the same Condor log file.
That Condor log file must be unique.  No other DAGs or Condor jobs can point
to that log file.
\item[Future Versions]: All jobs in a Dag must go to one log file, but
log file can be shared with other Dags and Condor jobs.
\end{description}

\begin{description}
\item[Feature]: Job UNDO
\item[Version 6.1.0]: All jobs must exit normally, else DAG will be aborted
\item[Future Versions]: A job can be ``undone'', or there is some
notion of a job instance.  Hence, a job that exits abnormally or is
cancelled by the user can be rerun such that the new run's log entry
is unique from the old run's log entry (in terms of recovery)
\end{description}

\begin{description}
\item[Feature]: Pre/Post Process
\item[Version 6.1.0]: Unsupported
\item[Future Versions]: A job can have a pre- and post-process script
specified, which are run before and after the job is submitted.  This can be
useful for performing tasks like compression or decompression or input or
output data.
\end{description}
