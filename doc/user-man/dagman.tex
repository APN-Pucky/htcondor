\subsection{Introduction}

The Directed Acyclic Graph Manager (DAGMan) is a software tool used with
Condor, a system developed at the University of Wisconsin. Condor executes
batch jobs utilizing cycles of idle workstations in the network. DAGMan is
responsible for submitting batch jobs in a predefined order and processing the
results. A configuration file is defined prior to execution of DAGMan in which
the job dependencies and appropriate executables is declared.

The importance of such a tool lies in the fact that the user is able to define
the execution order of a number of Condor Jobs. Just as Condor schedules
condor jobs, DAGMan schedules a system of jobs. In essence, it defines a
problem. Solving a problem may require multiple condor jobs that need data
from each other. This is best represented using a Directed Acyclic Graph
(DAG), which represents the flow of control from one node to another (i.e.,
from one condor job to another) through arrows.

From the point of view of the user, the scheduler is initialized with the
order of execution of jobs, and then started. DAGMan is responsible for all
scheduling, recovery and reporting activities of the submitted system of jobs.

\subsection{DAG Input File}

For Unix users, a useful analogy might be to think of the DAGMan input file as
a makefile, and DAGMan itself as the make executable.  However, DAGMan differs
from make.  Instead of looking at file modification timestamps, DAGMan reads
the Condor log file generated by each Condor job to find out which jobs are
unsubmitted, submitted, or complete.  DAGMan also makes a guarantee that a DAG
is recoverable, even if the machine running DAGMan goes down during execution.

\subsubsection{Description}

Job dependencies are defined prior to execution of the DAGMan program, using a
DAG input file.  An example input configuration file name is \File{abc.dag}.
The input file is read completely, and the DAG data structure is constructed
in memory before the first job is submitted.  With the exception of the
\textit{CondorCommandFile} (see below), the input file is case insensitive.

Throughout the input file, comments can be placed.  Legal comments exist on a
single line which immediately starts with a `\texttt{\#}' character, followed
by any characters up to the newline `\texttt{$\backslash$n}'.

It is interesting to note that the DAGMan input file does not contain any
specifics about the individual jobs. Each condor job by itself is handled as
if DAGMan was not present (this includes compiling and linking of the
job). The executable and the input/output parameters for each job are
contained in the CondorCommandFile.  The DAG file merely describes the
relationship between the different condor jobs using the semantics just
described.

\underline{Signature}

The first line of a DAG input file is the signature, which precisely
identifies which DAG file format follows.  As of this writing, only one DAG
format exists, and thus only one signature is possible.

\begin{verbatim}
  ### DAGMan 6.1.0
\end{verbatim}

This line must appear as it is written here, character for character.
Anything different will be rejected by DAGMan.  Having a precise signature tag
will enable future versions of DAGMan to remain backward compatible with older
DAG input file formats.

\underline{Job Section}

The Job Section of the input DAG file declares all the jobs that will appear
in the DAG.  Each job is described by a single line called a Job Entry.  The
following syntax is used:

\begin{verbatim}
	JOB <JobName> <CondorCommandFile>
\end{verbatim}

The \texttt{JOB} keyword (shown here in upper case only for clarity) declares
this line will map a \textit{JobName} to a Condor Command File.  The
\textit{JobName} is used by DAGMan to uniquely identify jobs throughout the
input file and to name them in output messages.  The
\textit{CondorCommandFile} is the input file used by \Condor{submit} to run
the individual condor job.  Because the Unix file system is case sensitive,
the case of the \textit{CondorCommandFile} is preserved.

The JobName can be any string that contains no white space.  The JobName is
not case sensitive, so ``JobA'' is equivalent to ``joba''.  An example
\textit{CondorCommandFile} name is \File{a.condor}.  Some important
restrictions are placed on the contents of the \textit{CondorCommandFile},
which will be discussed later.

The user can also have the option of declaring a job as being already
completed in the DAG input file. This may be useful in situations where the
user wishes to verify results, but does not need the entire job dependency
graph to be executed. This is done by adding the word "DONE" to the end of the
Job declaration line.

\begin{verbatim}
	JOB <JobName> <CondorCommandFile> DONE
\end{verbatim}

\underline{Dependency Section}

The dependency section of the DAG input file follows the Job Section and
describes the dependencies between the jobs listed in the Job Section.  The
notion of a ``parent'' and ``child'' job is introduced here.  A parent job
produces output which is required by one or more child jobs.  None of the
children can run until the parent successfully terminates.  A child job is one
whose input is taken from one or more parent jobs.  The child job cannot run
until all of its parents have successfully terminated.

A single line in the input file can specify the dependencies from one or more
parents to one or more children.

\begin{verbatim}
	PARENT <ParentJobName>* CHILD <ChildJobName>*
\end{verbatim}

The \texttt{PARENT} keyword is followed by one or more
\textit{ParentJobName}s.  Those are followed by the \texttt{CHILD} keyword,
which is followed by one or more \textit{ChildJobName}s.  Each child job
depends on each and every parent job on this line.  So the line
``\texttt{PARENT p1 p2 CHILD c1 c2}'' would produce four dependencies.

\subsubsection{Example}

The following \File{abc.dag} DAG input file shown below is illustrated with
the drawning that follows.

\begin{verbatim}
  ### DAGMan 6.1.0
  # Filename: abc.dag
  #
  Job  A  A.condor 
  Job  B  B.condor 
  Job  C  C.condor	
  PARENT A CHILD B C
  PARENT B CHILD C
\end{verbatim}

With \File{abc.dag}, the only possible execution is for Job A to execute,
followed by B, followed by C.  However, in a larger DAG, its very likely and
desirable for more than one job to be eligible for concurrent submission.
This can occur if all the parents of each submittable child job successfully
complete.

\subsection{Execution}

\subsubsection{Preparing Jobs}

Each individual job in a DAG is free to be a unique executable, with a unique
\textit{CondorCommandFile}.  The DAG can contain a mixture of standard and
vanilla jobs, or even other meta-scheduler jobs, like DAGMan.  On the other
hand, the jobs in the DAG could all use the same executable, or even the same
\textit{CondorCommandFile}.  Anything between both extremes is possible.
However, two limits are imposed.

First, each \textit{CondorCommandFile} must submit a cluster of size one.
There cannot be multiple \texttt{queue} lines.  The reasoning is long winded,
but to summarize quickly, the jobs from a multi-job cluster still produce
completely separate log events.  Thus, DAGMan cannot distinguish between two
jobs from the same cluster.  This restriction will probably never be lifted.

Second, all \textit{CondorCommandFile}s of a DAG must specify the same log.
In order for DAGMan to follow the order of events correctly, all events from
all jobs in the DAG must be sent to the same log file.  This restriction will
be loosened in later versions (see section~\ref{dagman:version}).

For this example, we will write a single \textit{CondorCommandFile} to be used
by all three jobs in the DAG.  Thus, each job will run the same executable.
This example is very artificial, because normally separate jobs would need
output for their child jobs to go to unique output and error files.
Otherwise, the jobs would be clobbering each other's output.  However, since
we are sending output and error to \File{/dev/null}, sharing the
\textit{CondorCommandFile} is OK.

\begin{verbatim}
  # Filename: abc_job.condor
  #
  executable   = /path/abc.exe
  output       = /dev/null
  error        = /dev/null
  log          = abc_condor.log
  universe     = vanilla
  notification = NEVER
  queue
\end{verbatim}

Note that notification is set to \texttt{NEVER}.  This is recommended if you
prefer not to have Condor send you e-mail for every job in a large DAG.

\subsubsection{Writing the DAG File}

The DAG file names the jobs, associates jobs with their
\textit{CondorCommandFile}, and declares job dependencies.  For our artificial
ABC example, all three jobs will use the same abc\_job.condor file written
earlier.  However, a more typical DAG file would have unique
\textit{CondorCommandFile} for every job.

\begin{verbatim}
  # Filename: abc.dag
  # ABC DAG File for DAGMan
  #
  Job  A  abc_job.condor
  Job  B  abc_job.condor
  Job  C  abc_job.condor
  PARENT A CHILD B C
  PARENT B CHILD C
\end{verbatim}

This DAG file will be the input file for the \Condor{dagman} program.

\subsubsection{Submitting the DAG to Condor}

In order to guarantee recoverability, the DAGMan program itself is run as a
Condor job.  However, DAGMan is not submitted as a standard universe or
vanilla universe job.  Instead, it is run as a meta-scheduler.  Standard and
vanilla universe jobs are usually submitted to the local schedd, which
schedules them for execution on some remote machine in the pool that is idle.
A meta-scheduler is also submitted to the local schedd, but runs on the local
schedd.  The meta-scheduler then submits jobs, according to its design, to the
same local schedd, just as if the user submitted them manually.  In fact, the
local schedd does not know the difference between DAGMan submitting a job, and
the user who originally submitted DAGMan, and could have submitted the DAG
jobs manually.

Here is an example \textit{CondorCommandFile} that would be used to submit the
ABC DAG from the previous section.

\begin{verbatim}
  # Filename: abc.dag.condor
  # Condor Command File to submit the DAGMan Program
  # Submit with:  condor_submit abc_dag.condor
  #
  executable   = /path/condor_dagman
  output       = abc.dag.stdout
  error        = abc.dag.stderr
  log          = abc.dag.log
  arguments    = -f -t -l . -lockfile abc.dag.lock -condorlog abc.dag.condor.log -dag abc.dag
  universe     = scheduler
  notification = ALWAYS
  queue
\end{verbatim}

The full executable path of \Condor{dagman} is specified.  The
\texttt{output=} is highly recommended, as this version of DAGMan (see
section~\ref{dagman:version}) does not yet implement a command socket for
status updates.  All progress is reported via stdout, which will change in
later versions.

The \texttt{arguments=} line is quite loaded and will be reviewed here.  The
first few arguments are specifically for daemon core, to which DAGMan is
linked.  Daemon Core recognizes \texttt{-f} to mean, run in the forground.
The \texttt{-t} option sends error messages to stderr instead of a log file.
Finally, ``\texttt{-l .}'' overrides the configuration file's log directory.
With the -t option, a log file is not necessary, but overriding the log
directory frees us from having to write daemon core configuration files.  The
remaining options are for DAGMan itself.  The lockfile is created as a flag
that a DAG is not yet completed.  If DAGMan is killed during the run of a DAG,
it knows to recover by checking the existence of the lockfile.  Each DAG must
be given a unique lock file.  The Condor Log is the log file to which every
job in the DAG must write to.  This limitation is imposed to avoid the
difficult task of reading multiple log files concurrently.  This first version
of DAGMan also imposes the limit that the log file must be unique, meaning
that no other DAGs or Condor jobs can write to this log file.  Future versions
of DAGMan will allow multiple DAGs and Condor jobs to write to the same log
file, and DAGMan's recovery engine will be able to distinguish them (see
section~\ref{dagman:version}).  Finally, the DAG input file is specified to be
our ABC DAG, \File{abc.dag}.

The \texttt{universe=scheduler} declares that DAGMan is not a typical Condor
job, but rather a meta-scheduler.  Meta schedulers are run on the local
schedd, rather than being submitted to a remote host.

To run the DAG, issue the command \Condor{submit} abc.dag.condor.  Because
\texttt{notification=ALWAYS} is specified, Condor will send a single e-mail
message if something significant (successful completion or error) happens to
the DAG as a whole.

\subsection{Removal}

After submitting a DAG, the user may change her mind and wish to remove the
entire DAG, plus any jobs submitted by that DAG which happen to currently be
running.  DAG removal is easily accomplished by issuing a \Condor{rm} on the
DAGMan job itself.  The schedd sends a special signal to the meta-scheduler,
telling it to remove any of its condor jobs (using \Condor{rm}) that are
currently running.

However, if the machine is scheduled to go down, and the schedd receives a
shutdown command from the master, the schedd will send a running DAGMan job a
similar shutdown, which instructs DAGMan to clean up memory and exit.
However, in this case, DAGMan does not remove its submitted jobs, but rather
expects them to persistantly exist in the Condor queue after restart.

The important thing to remember is that DAGMan will not explicitely run
\Condor{rm} on its jobs except as a result of the user running \Condor{rm} on
the DAGMan job.

\subsection{Recovery}

The Condor system offers the benefit of recoverability, in that if any host
crashes, Condor jobs that were running can be recovered, either by continuing
from the last checkpoint, or rerunning from scratch.  In any event, Condor
guarantees that once a job is successfully submitted, the Condor system will
not loose it.

DAGMan makes the same guarantee about the DAG as a whole.  If the machine
running DAGMan goes down or crashes, upon restart DAGMan will be restarted,
and the state of the DAG jobs will be recovered from the log file
(\File{abc.dag.condor.log} from our example before).

\subsection{Version Summary}
\label{dagman:version}

This section addresses the features and limitations that exist in the current
version of DAGMan, and how they may change in future versions.

This first public release of DAGMan was written and tested in the Condor 6.1.0
environment.  It is shipped seperate from the main Condor system as a
contribution program.  As such, it is not as rigorously tested as the core
components of Condor.  A reasonable effort has been made to test large DAGS
(on the order of 5000 jobs) on Solaris x86 and Sparc.  However, the DAGMan is
not arrogant enough to claim itself bug free.  Users are encouraged to send
e-mail to \texttt{}.............

The following feature summary compares the current version with possible
versions of DAGMan still to come.

\begin{description}
\item[Feature] : Command Socket
\item[Version 6.1.0] : Unsupported
\item[Future Versions] : A general purpose command socket will be used to
direct Dagman while it's running.  Commands like CANCEL\_JOB X or DELETE\_ALL
would be supported, as well as notification messages like JOB\_SUBMIT or
JOB\_TERMINATE, etc.  Eventually, a Java Gui would graphically represent the
Dag's state, and offer buttons and dials for graphic Dag manipulation.
\end{description}

\begin{description}
\item[Feature]: DAG removal
\item[Version 6.1.0]: Supported via \Condor{rm} of the DAG.
\item[Future Versions]: Supported by a command socket such as DELETE\_ALL
\end{description}

\begin{description}
\item[Feature]: Condor Log File
\item[Version 6.1.0]: All jobs in a DAG must specify the same Condor log file.
That Condor log file must be unique.  No other DAGs or Condor jobs can point
to that log file.
\item[Future Versions]: All jobs in a Dag must go to one log file, but
log file can be shared with other Dags and Condor jobs.
\end{description}

\begin{description}
\item[Feature]: Job UNDO
\item[Version 6.1.0]: All jobs must exit normally, else DAG will be aborted
\item[Future Versions]: A job can be ``undone'', or there is some
notion of a job instance.  Hence, a job that exits abnormally or is
cancelled by the user can be rerun such that the new run's log entry
is unique from the old run's log entry (in terms of recovery)
\end{description}

\begin{description}
\item[Feature]: Pre/Post Process
\item[Version 6.1.0]: Unsupported
\item[Future Versions]: A job can have a pre- and post-process script
specified, which are run before and after the job is submitted.  This can be
useful for performing tasks like compression or decompression or input or
output data.
\end{description}
