\documentstyle{article}

\input{../condor-macros}

\begin{document}

\section{pvm}

Condor has a pvm universe which allows the user to submit pvm jobs to
the Condor pool.  In this section, we will first
show the difference between the normal pvm and the pvm under condor
environment.  Then we give some hints on how to write good pvm
programs to suit the condor environment by an example program.  In the
end, we illustrate how to submit pvm jobs to condor by examining a
sample pvm submit file.

\subsection{The differences between Condor-pvm and normal pvm}

This release of the Condor-pvm is based on pvm3.3.11.  Most of the pvm
library functions under Condor maintain the same semantics as in
pvm3.3.11, including the group operations and pvm\_catchout().

We summarize the changes and new features of pvm under condor in the
following list.
\begin{itemize}

\item Concept of machine class.  Under Condor-pvm, machines of
  different attributes belong to different machine classes.  Machine
  classes are are numbered 0, 1, ..., etc..  A machine class can be
  specified by the user in the submit file (see section~\ref{submit}
  for details).

\item \func{pvm\_addhosts()}.  When the application
  needs to add a host machine, it should call \func{pvm\_addhosts()}
  with the first argument as a string that specifies the machine
  class.  For example, to specify class 0, a pointer to string ``0''
  should be used as the first argument.  Condor will find a machine
  that satisfies the requirements of class 0 and adds it to the pvm
  virtue machine.

  Under condor, \func{pvm\_addhost()} is no longer synchronous.  It
  will return before the hosts are actually added to the virtue
  machine.  The user should call \func{pvm\_notify()} before calling
  \func{pvm\_addhost()}, so that when a host is added later, the user
  will be notified.
	
\item \func{pvm\_notify()}.  Under condor, we added two additional 
  possible notification requests, \func{PvmHostSuspend} and
  \func{PvmHostResume}, to the function \func{pvm\_notify()}.  When a
  host is suspended (or resumed) by condor, if the user has called
  \func{pvm\_notify()} with that host tid and with
  \func{PvmHostSuspend} (or \func{PvmHostResume}) as arguments, then
  the application will receive a notification for the corresponding
  event.

\item \func{pvm\_spawn()}.  If  the flag in \func{pvm\_spawn()} is 
  PvmTaskArch, then the string specifying the desired architecture
  class should be used.  Currently, under condor, we allow only one
  user task to run on one machine, since condor will suspend or vacate
  a job if the load on its machine is higher than a specified
  threshold.

\end{itemize}

\subsection{An example}

Normal pvm applications assume dedicated machines.  However, running a
pvm application under Condor, since Condor environment is an
opportunistic environment, machines can be suspended and even removed
from the pvm virtue machine during the life time of the pvm
application.  

Here, we include an extensively commented skeleton of a sample pvm
program \Prog{master\_sum.c}, which, we hope, will help the users to
write pvm code that is better suited for the Condor environment.

\begin{small}
\begin{verbatim}
/* 
 * master_sum.c
 *
 * master program to perform parallel addition - takes a number n 
 * as input and returns the result of the sum 0..(n-1).  Addition 
 * is performed in parallel by k tasks, where k is also taken as 
 * input.  The numbers 0..(n-1) are stored in an array, and each 
 * worker adds a portion of the array, and returns the sum to the 
 * master.  The Master adds these sums and prints final sum.  
 *
 * To make the program fault-tolerant, the master has to monitor 
 * the tasks that exited without sending the result back.  The 
 * master creates some new tasks to do the work of those tasks 
 * that have exited. 
 */

#define NOTIFY_NUM 5  /* number of items to notify */

#define HOSTDELETE 12
#define HOSTSUSPEND 13
#define HOSTRESUME 14
#define TASKEXIT 15
#define HOSTADD 16
	
/* send the pertask and start number to the worker task i */
void send_data_to_worker(int i, int *tid, int *num, int pertask, 
			FILE *fp, int round)
{
     int status;
     int start_val;
     
     /* send the round number */
     pvm_initsend(PvmDataDefault); /* XDR format */
     pvm_pkint(&round, 1, 1);    /* number of numbers to add */
     status = pvm_send(tid[i], ROUND_TAG);

     pvm_initsend(PvmDataDefault); /* XDR format */
     pvm_pkint(&pertask, 1, 1);    /* number of numbers to add */
     status = pvm_send(tid[i], NUM_NUM_TAG);

     pvm_initsend(PvmDataDefault); /* XDR format */
     start_val = i * pertask; /* initial number for this task */
     pvm_pkint(&start_val, 1, 1);     /* the initial number */
     status = pvm_send(tid[i], START_NUM_TAG);   

     fprintf(fp, "Round %d: Send data %d to worker task %d, ``
	``tid =%x. status %d \n", round, start_val, i, 
	tid[i], status);
}

/* 
 * to see if more hosts are needed 
 * 1 = yes; 0 = no 
 */
int need_more_hosts(int i)
{
     int nhost, narch;
     char *hosts="0";  /* any host in arch class 0 */
     struct pvmhostinfo *hostp = (struct pvmhostinfo *) 
	  calloc (1, sizeof(struct pvmhostinfo));

     /* get the current configuration */
     pvm_config(&nhost, &narch, &hostp);
     
     if (nhost > i)
	  return 0;
     else 
	  return 1;
}

/* 
 * Add a new host until success, assuming that request for 
 * PvmAddHost notification has already been sent 
 */
void add_a_host(FILE *fp)
{
     int done = 0;
     int buf_id;
     int success = 0;
     int tid;
     int msg_len, msg_tag, msg_src;
     char *hosts="0";  /* any host in arch class 0 */
     int infos[1];

     while (done != 1) {
	  /* 
	   * add one host - no specific machine named 
	   * add host will asynchronously, so we need
	   * to receive the notification before go on.
	   */
	  pvm_addhosts(&hosts,1 , infos);
	  
	  /* receive hostadd notification from anyone */
	  buf_id = pvm_recv(-1, HOSTADD);
	  
	  if (buf_id < 0) {
	       fprintf(fp, "Error with buf_id = %d\n", buf_id);
	       done = 0;
	       continue;
	  }
	  done = 1;
     
	  pvm_bufinfo(buf_id, &msg_len , &msg_tag, &msg_src);
	  pvm_upkint(&tid, 1, 1);

	  pvm_notify(PvmHostDelete, HOSTDELETE, 1, &tid);

	  fprintf(fp, "Received HOSTADD: ");
	  fprintf(fp, "Host %x added from %x\n", tid, msg_src);
	  fflush(fp);
     }
}

/* 
 * Spawn a worker task until success.  
 * Return its tid, and the tid of its host. 
 */
void spawn_a_worker(int i, int* tid, int * host_tid, FILE *fp)
{
     int numt = 0;
     int status;

     while (numt == 0){
	  /* spawn a worker on a host belonging to arch class 0 */
	  numt = pvm_spawn ("worker_sum", NULL, PvmTaskArch, "0",
			1, &tid[i]);

	  fprintf(fp, "master spawned %d task tid[%d] = %x\n" , 
		  numt, i , tid[i]);
	  fflush(fp);
     
	  /* if the spawn is successful */
	  if (numt == 1)
	  {
	       /* notify when the task exits */
	       status = pvm_notify(PvmTaskExit, TASKEXIT, 1, &tid[i]);
	       
	       fprintf(fp, "Notify status for exit = %d\n", status);
	       
	       if (pvm_pstat(tid[i]) != PvmOk)
		    numt = 0;
	  }
	  
	  if (numt != 1)
	  {
	       fprintf(fp, "!! Failed to spawn task[%d]\n", i);
	       
	       /* 
		* currently condor-pvm allows only one task running on 
		* a host
		*/
	       while (need_more_hosts(i) == 1)
		    add_a_host(fp);
	  }
     }
}


main()
{
     int n;        /* will add <n> numbers n .. n-1 */
     int ntasks;   /* need <ntask> workers to do the addition. */
     int pertask;  /* numbers to add per task */

     int tid[MAX_TASKS];     
     int deltid[MAX_TASKS];  /* tids monitored for deletion */
     
     int sum[MAX_TASKS];     /* hold the reported sum */
     int num[MAX_TASKS];     /* the initail numbers the workers 
				should add */

     int host_tid[MAX_TASKS];  /* the tids of the host that the *
				* tasks <0..ntasks> are running on*/
     
     int i, numt, nhost, narch, status;
     int result;
     int mytid;    /* task id of master */
     int mypid;    /* process id of master */
     int buf_id;   /* id of recv buffer */
     int msg_leg, msg_tag, msg_src, msg_len;
     int int_val;  

     int infos[MAX_TASKS];
     char * hosts[MAX_TASKS];
     struct pvmhostinfo *hostp = (struct pvmhostinfo *) 
	calloc (MAX_TASKS, sizeof(struct pvmhostinfo));

     FILE *fp;
     char outfile_name[100];

     char *codes[NOTIFY_NUM] = {"HostDelete", "HostSuspend", 
			"HostResume", "TaskExit", "HostAdd"};
     
     int count;   /* the number of times that while loops */
     int round_val;
     int correct = 0;
     int wrong = 0;

     mypid = getpid();

     sprintf(outfile_name, "out_sum.%d", mypid);
     fp = fopen(outfile_name, "w"); 

     /* redirect all children tasks' stdout to fp */
     pvm_catchout(stderr);  

     if (pvm_parent() == PvmNoParent){
	  fprintf(fp, "I have no parent!\n");
	  fflush(fp);
     }
     /* will add <n> numbers 0..(n-1) */
     fprintf(fp, "How many numbers? ");
     fflush(fp);
     scanf("%d", &n);
     fprintf(fp, "%d\n", n);
     fflush(fp);

     /* will spawn ntasks workers to perform addition */
     fprintf(fp, "How many tasks? ");
     fflush(fp);
     scanf("%d", &ntasks);
     fprintf(fp, "%d\n\n", ntasks);
     fflush(fp);

     /* will iterate count loops */
     fprintf(fp, "How many loops? ");
     fflush(fp);
     scanf("%d", &count);
     fprintf(fp, "%d\n", count);
     fflush(fp);

     /* set the hosts to be in arch class 0 */
     for (i = 0; i< ntasks; i++) 
	  hosts[i] = "0";

     /* numbers to be added by each worker */
     pertask = n/ntasks;

     /* get the master's TID */
     mytid = pvm_mytid();
     fprintf(fp, "mytid = %x; mypid = %d\n", mytid, mypid);

     /* get the current configuration */
     pvm_config(&nhost, &narch, &hostp);

     fprintf(fp, "current number of hosts = %d\n", nhost);
     fflush(fp);

     /* 
      * notify request for host addition, with tag HOSTADD, 
      * no tids to monitor.  
      *
      * -1 turns the notification request on;
      * 0 turns it off;
      * a positive integer n will generate at most n 
      * notifications.
      */     
     pvm_notify(PvmHostAdd, HOSTADD, -1, NULL);

     /* add more hosts - no specific machine named */
     i = ntasks - nhost;
     if (i > 0) {
	  status = pvm_addhosts(hosts, i , infos);
	  
	  fprintf(fp, "master: addhost status = %d\n", status);
	  fflush(fp);
     }
     
     /* if not enough hosts, loop and call pvm_addhosts */
     for (i = nhost; i < ntasks; i++)
     {
	  /* receive notification from anyone, with HostAdd tag */
	  buf_id = pvm_recv(-1, HOSTADD);

	  if (buf_id < 0) {
	       fprintf(fp, "Error with buf_id = %d\n", buf_id);
	  }
	  else {
	       fprintf(fp, "Success with buf_id = %d\n", buf_id);
	  }

	  pvm_bufinfo(buf_id, &msg_len , &msg_tag, &msg_src);
	  if (msg_tag==HOSTADD)
	  {
	       pvm_upkint(&int_val, 1, 1);

	       fprintf(fp, "Received HOSTADD: ");
	       fprintf(fp, "Host %x added from %x\n", 
			int_val, msg_src);
	       fflush(fp);
	  }
	  else
	       fprintf(fp, "Received unexpected message with tag: %d\n", 
		       msg_tag);

     }

     /* get current configuration */
     pvm_config(&nhost, &narch, &hostp);

     /* notify all exceptional conditions about the hosts*/
     status = pvm_notify(PvmHostDelete, HOSTDELETE, 
		ntasks, deltid);
     fprintf(fp, "Notify status for delete = %d\n", status);
     
     status = pvm_notify(PvmHostSuspend, HOSTSUSPEND, 
		ntasks, deltid);
     fprintf(fp, "Notify status for suspend = %d\n", status);
     
     status = pvm_notify(PvmHostResume, HOSTRESUME, 
		ntasks, deltid);
     fprintf(fp, "Notify status for resume = %d\n", status);

     /* spawn <ntasks> */
     for (i = 0; i < ntasks ; i++) 
     {
	  /* spawn the i-th task, with notifications. */
	  spawn_a_worker(i, tid, host_tid, fp);
     }

     /* add the result <count> times */
     while (count > 0) 
     {
	  /* 
	   * if array length was not perfectly divisible by ntasks, 
	   *	some numbers are remaining. Add these yourself 
	   */
	  result = 0;
	  for ( i = ntasks * pertask ; i < n ; i++)
	       result += i;
	 
	  /* initialize the sum array with -1 */
	  for (i = 0; i< ntasks; i++) 
	    sum[i] = -1;
 
	  /* send array partitions to each task */
	  for (i = 0; i < ntasks ; i++) 
	  {
	       send_data_to_worker(i, tid, num, pertask, fp, count);
	  }

	  
	  /* 
	   * Wait for results.  If a task exited without 
	   * sending back the result, start another task to do
	   * its job. 
	   */
	  
	  for (i = 0; i< ntasks; )
	  {	  
	       buf_id = pvm_recv(-1, -1);
	       pvm_bufinfo(buf_id, &msg_len , &msg_tag, &msg_src);
	       fprintf(fp, "Receive: task %x returns mesg tag %d, ``
			``buf_id = %d\n", 
		       msg_src, msg_tag, buf_id);
	       fflush(fp);
	       
	       /* is a result returned by a worker */
	       if(msg_tag == RESULT_TAG)  
	       {
		    int j;
		    
		    pvm_upkint(&round_val, 1, 1);
		    fprintf(fp, "  round_val = %d\n", round_val);
		    fflush(fp);
		 
		    if (round_val != count)
			 continue;

		    pvm_upkint(&int_val, 1, 1);
		    for (j=0; (j<ntasks) && (tid[j] != msg_src); j++)
			 ;
		    fprintf(fp, "  Data from task %d, tid = %x : %d\n", 
			    j, msg_src, int_val);
		    fflush(fp);
		    
		    if (sum[j] == -1) {
			 sum[j] = int_val; /* store the sum */
			 i++;
		    }
	       }
	       /* A task has exited. */
	       else if (msg_tag == TASKEXIT) 
	       {
		    /* Find out which task has exited. */ 
		    int which_tid, j;	       
		    pvm_upkint(&which_tid, 1, 1);
		    for (j=0; (j<ntasks) && (tid[j] != which_tid); j++)
			 ;
		    fprintf(fp, "  from tid %x : task %d, tid =  %x, ``
			    ``exited.\n", 
			    msg_src, j, which_tid);
		    fflush(fp);
		    /* 
		     * If a task exited before sending back the message,
		     * create another task to do the same job.
		     */
		    if (j < ntasks && sum[j] == -1) 
		    {
			 /* spawn the j-th task */
			 spawn_a_worker(j, tid, host_tid, fp);
			 
			 /* send unfinished work to the new task */
			 send_data_to_worker(j, tid, num, 
					pertask, fp, count);
		    }
	       }
	       
	       /* 
		* If a host has been deleted, check to see if 
		* the tasks running on it has been finished.  
		* If not, should create  new worker tasks to do 
		* the work on some other  hosts.
		*/
	       else if (msg_tag == HOSTDELETE)
	       {
		    int which_tid, j;
		    
		    /* get which host has been suspended/deleted */
		    pvm_upkint(&which_tid, 1, 1);
		    
		    fprintf(fp, "  from tid %x : %x %s\n", 
			    msg_src, which_tid, 
			    codes[msg_tag - HOSTDELETE]);
		    fflush(fp);
		    
		    /* 
		     * If the task on that host has not finished its
		     * work, then create new task to do the work.
		     */
		    for (j = 0; j < ntasks; j++)
			 
			 if (host_tid[j] == which_tid && sum[j] == -1)
			 {
			      fprintf(fp, "host_tid[%d] = %x, ``
					``need new task\n",
				      j, host_tid[j]);
			      fflush(fp);
			      
			      /* spawn the i-th task, with notifications. */
			      spawn_a_worker(j, tid, host_tid, fp);
			      
			      /* send the unfinished work to the new task */
			      send_data_to_worker(j, tid, num, 
					pertask, fp, count);
			      
			 }
	       }
	       
	       /* print out some other notifications or messages */
	       else 
	       {
		    int which_tid;
		    pvm_upkint(&which_tid, 1, 1);
		    
		    fprintf(fp, "  from tid %x : %x %s\n", msg_src,
			 which_tid,   codes[msg_tag - HOSTDELETE]);
		    fflush(fp);
	       }
	  }	       
	  
	  /* add up the sum */
	  for (i=0; i<ntasks; i++)
	       result += sum[i];
	  
	  fprintf(fp, "Sum from  0 to %d is %d\n", n-1 , result);
	  fflush(fp);
	  
	  /* check correctness */
	  if (result == (n-1)*n/2)
	  {
	       correct++;
	       fprintf(fp, "*** Result Correct! ***\n");
	  }
	  else {
	       wrong++;
	       fprintf(fp, "*** Result WRONG! ***\n");
	  }

	  fflush(fp);
	  
	  count--;
     }
     
     fprintf(fp, "correct = %d; wrong = %d\n", correct, wrong);
     fflush(fp);

     pvm_exit();
     exit(0);
}

\end{verbatim}
\end{small}

\subsection{Sample submit file}
\label{submit}

Like submitting jobs in any other universe,
to submit a pvm job, the user needs to specify the requirements and
options in the submit file.  Figure~\ref{pvm_submit} is an example of
a submit file of a pvm job.  This job has a master pvm program called
\func{master\_pvm}.


\begin{figure}[ht]
\begin{small}
\begin{verbatim}
###########################################################
# sample_submit
# Sample submit file for pvm jobs. 
###########################################################

# The job is a pvm universe job.
universe = PVM  

# The executable of the master pvm program is ``master_pvm''.
executable = master_pvm

In = "in_sum"
Out = "stdout_sum"
Err = "err_sum"

###################  Architecture class 0  ##################

Requirements = (Arch == "INTEL") && (OpSys == "SOLARIS251") 

# We want at least 2 machines in class 0 before starting the 
# program.  We can use up to 4 machines.
machine_count = 2..4  
queue

###################  Architecture class 1  ##################

Requirements = (Arch == "SUN4M") && (OpSys == "SOLARIS251") 

# We need at least 1 machine in class 1 before starting the 
# executable.  We can use up to 3 to start with.
machine_count = 1..3
queue

###############################################################
# note: the program will not be started until the least 
#       requirements in all classes are satisfied.
###############################################################
\end{verbatim}
\end{small}

\label{pvm_submit}
\caption{A sample submit file for pvm jobs.}
\end{figure}

In this sample submit file, the command \func{universe = PVM}
specifies that the jobs should be submitted into pvm universe.

The command \func{executable = master\_pvm} tells Condor that the pvm
master program is \func{master\_sum}.  This program will be started on
the submitting machine.  The workers should be spawned by this master
program during execution.

This submit file also tells condor that the pvm virtue machine is
consisted of two different classes of machine architectures.  Class
0 contains machines with INTEL architecture running SOLARIS251; class
1 contains machines with SUN4M architecture running SOLARIS251.

By using \func{machine\_count = <min>..<max>}, the submit file tells
Condor that to start the pvm program, there should be at least <min>
number of machines of the current class.  It also asks Condor to give
it as many as <max> machines.  During the execution of the program,
the application can get more machines of each of the class by calling
\func{pvm\_addhosts()} with a string specifying the desired architecture
class.  (See the sample program in this section for details.)

The \func{queue} command should be inserted after the specifications of
each class.




\end{document}
