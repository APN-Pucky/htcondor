% Handy macros.
%\newcommand{\Keyword}{\Prog{#1}}
\newcommand{\TBD}{TBD}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\label{sec:Stork}Stork Applications}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\index{Stork|(}

Today's scientific applications have huge data requirements which continue to
increase drastically every year. These data are generally accessed by many
users from all across the the globe. This requires moving huge amounts of data
around wide area networks to complete the computation cycle, which brings with
it the problem of efficient and reliable data placement.

Stork is a scheduler for data placement.  With Stork, \Term{data placement
jobs} have been elevated to the same level as computational jobs:  data
placements may be queued, managed, queried and even autonomously restarted upon
error.  Stork understands the semantics and protocols of data placement.
Modules are available for transferring 
data via the following protocols:

\begin{table}[hbt]
	\begin{tabular}{ l l }
		file:/		& (Stork Server) local file system \\
		ftp://		& FTP File Transfer Protocol \\
		http://		& HTTP Hypertext Transfer Protocol \\
		gsiftp://	& Globus 
		\htmladdnormallink{GridFTP}{http://www.globus.org/datagrid/gridftp.html}
		\\
		nest://		& Condor \htmladdnormallink{NeST}
			{ http://www.cs.wisc.edu/condor/nest/} network storage appliance \\
		srb://		& SDSC \htmladdnormallink{SRB}{http://www.sdsc.edu/srb/}
			Storage Resource Broker \\
		srm://		& \htmladdnormallink{SRM}{http://sdm.lbl.gov/srm-wg/}
			Storage Resource Manager \\
		unitree://    & NCSA \htmladdnormallink{UniTree}
			{http://www.ncsa.uiuc.edu/Divisions/CC/HPDM/unitree/} \\
%		diskrouter:// -> UW DiskRouter Tool
	\end{tabular}
\end{table}

Further, the architecture is \emph{modular}.  Users are free to create and use
their own modules.

Stork includes high level features for managing data transfers.  First, users
can throttle the number of active jobs running from a Stork server.  Second,
Stork includes builtin fault tolerance, with capabilities for retrying failed
jobs, and using arbitrary lists of alternate protocols.

Finally, Stork users also have access to a higher level job manager, 
Condor DAGMan (section \ref{sec:DAGMan}), which can manage both Stork data
placement jobs, and traditional CPU processing jobs.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\label{sec:Stork-Job-Submission}Submitting Stork Jobs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\index{Stork!submit description file}

As with Condor jobs, Stork jobs are specified with a \Term{submit description
file}.  It is important to note the syntax of the submit description file for a
Stork job is \emph{different} than that used by Condor jobs.  Specifically,
Stork submit description files are written in the
\htmladdnormallink{ClassAd}{http://www.cs.wisc.edu/condor/classad} Language.
See the ClassAd documentation for syntax and details.  Stork then defines
several keywords, which when present in the job submit file, define the
function of the job.

Here is sample, basic Stork job submit file, showing file syntax and important
keywords.  Each job specifies a 1-to-1 mapping of a data source URL to
destination URL.

\begin{verbatim}
	// This is a comment line.
	[
		dap_type = transfer;
		src_url = "file:/etc/termcap";
		dest_url = "gsiftp:/tmp/stork/file-termcap";
	]
\end{verbatim}

This example shows the keyword/value pairs that form the heart of a Stork job
specification.  The minimum keywords required to specify a Stork job are:

\begin{description}
	\item[dap\_type] Currently, the data type is constrained to 
	\Keyword{transfer}.

	\item[src\_url]  Specify the data source URL.

	\item[dest\_url]  Specify the data destination URL.

\end{description}

Additionally, the following keywords may be use in a job submit file:

\begin{description}
	\item[x509proxy] Specify the location of the X.509 proxy file for protocols
    that require GSI authentication, such as gsiftp://.  future versions of
    Stork may no longer require this keyword, and will look for the proxy in
    the default locations.  Until then, this keyword is still needed.  The
    default value is null.

    \item[alt\_protocols] Specify a list of retry protocol pairs, to be used if
    the transfer protocols specified via the \Keyword{src\_url} and
    \Keyword{dest\_url} keywords fails.  The default value is null.  See
    section \ref{sec:Stork-Fault-Protection} for a detailed discussion of Stork
    fault protection and alternate protocol retries.

    The syntax for specifying the alternate protocols list is:

    \texttt{alt\_protocols = }
        source\_protocoln-dest\_protocoln,
        \ldots

%	\item[dest\_host] \TBD
%	\item[reserve\_size] \TBD
%	\item[reserve\_id] \TBD

\end{description}

Stork places no restriction on the submit file name or extension, and will
accept any valid filename for a submit file.  Use of added whitespace and
comments for clarity are encouraged.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\label{sec:Stork-Job-Management}Managing Stork Jobs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Stork provides a set of command line user tools for job management, including
submitting, querying, and removing data placement jobs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\label{sec:stork-submit}Submitting Stork Jobs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Submit data placement jobs to Stork using the \Prog{stork\_submit} tool.  For
example, after creating the submit description file \texttt{sample.stork}with an
editor, submit the file with the command:

\begin{verbatim}
stork_submit sample.stork
\end{verbatim}

Stork then returns the associated job id, which can be used by other Stork
job control tools, below.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\label{sec:stork-query}Querying Stork Jobs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Use \Prog{stork\_status} to check the status of any active or completed Stork
job id.  \Prog{stork\_status} takes a single argument, the job id to query.
For example, to check the status of job id 1:

\begin{verbatim}
stork_status 1
\end{verbatim}

Use \Prog{stork\_q} to query all active Stork jobs.  
\Prog{stork\_q} does not [yet] take any command line arguments, however, query
filters may be added in the future.  
\Prog{stork\_q} does not report on completed Stork jobs.

For example, to check the status all active jobs:

\begin{verbatim}
stork_q
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\label{sec:stork-rm}Removing Stork Jobs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Active jobs may be removed from the job queue with the 
\Prog{stork\_rm} tool.  
\Prog{stork\_rm} takes a single argument, the job id to remove.  All jobs may
be removed, provided they have not completed.

For example, to remove queued job 1:

\begin{verbatim}
stork_rm 1
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\label{sec:Stork-Fault-Protection}Fault Protection}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In an ideal world, all data transfers succeed on the first attempt.  However,
we all know that data transfers can fail, for various reasons.  Stork is
designed with builtin data transfer fault protection, and can retry failed data
transfer jobs using a variety of protocols.  Specifically, Stork loops on each
transfer job, attempting in order:
\begin{itemize}
    \item Transfer protocols specified via job \Keyword{src\_url} and
        \Keyword{dest\_url} keywords.
    \item Any alternate protocols specified in the \Keyword{alt\_protocols}
    keyword, in exactly the order specified.
\end{itemize}

Stork will continue to retry as long as the transfer fails, or until the 
transfer retry limit specified via the 
\textsc{STORK\_MAX\_RETRY} parameter, section
\ref{param:StorkMaxRetry} is reached.  

It is important to note that for each retry, Stork will vary only the protocol
specified in either the \Keyword{src\_url} or \Keyword{dest\_url}.  All other
components of the URL are left unchanged, including the server, directory and
file.

Several examples will illustrate:

Given the default 
\textsc{STORK\_MAX\_RETRY} value of 10, and the sample job submit description
file without an \Keyword{alt\_protocols} entry:
\begin{verbatim}
    [
        dap_type = transfer;
        src_url = "proto1:/server-a/dir-a/file-a";
        src_url = "proto2:/server-b/dir-b/file-b";
    ]
\end{verbatim}

Stork will attempt up to 10 transfers from from 
			proto1:/server-a/dir-a/file-a
            to
			proto2:/server-b/dir-b/file-b .

before failing the job:

However, given the default 
\textsc{STORK\_MAX\_RETRY} value of 10, and the sample job submit description
file with an \Keyword{alt\_protocols} entry:
	\begin{verbatim}
		[
			dap_type = transfer;
			src_url = "proto1:/server-a/dir-a/file-a";
			src_url = "proto2:/server-b/dir-b/file-b";
			alt_protocols = "proto1-proto3, proto4-proto5";
		]
	\end{verbatim}

Stork will attempt the following transfer list, until a transfer succeeds,
before failing the job:
\begin{enumerate}
    \item transfer from 
			proto1:/server-a/dir-a/file-a
            to
			proto2:/server-b/dir-b/file-b
    \item transfer from 
			proto1:/server-a/dir-a/file-a
            to
			proto3:/server-b/dir-b/file-b
    \item transfer from 
			proto4:/server-a/dir-a/file-a
            to
			proto5:/server-b/dir-b/file-b
    \item transfer from 
			proto1:/server-a/dir-a/file-a
            to
			proto2:/server-b/dir-b/file-b
    \item transfer from 
			proto1:/server-a/dir-a/file-a
            to
			proto3:/server-b/dir-b/file-b
    \item transfer from 
			proto4:/server-a/dir-a/file-a
            to
			proto5:/server-b/dir-b/file-b
    \item transfer from 
			proto1:/server-a/dir-a/file-a
            to
			proto2:/server-b/dir-b/file-b
    \item transfer from 
			proto1:/server-a/dir-a/file-a
            to
			proto3:/server-b/dir-b/file-b
    \item transfer from 
			proto4:/server-a/dir-a/file-a
            to
			proto5:/server-b/dir-b/file-b
    \item transfer from 
			proto1:/server-a/dir-a/file-a
            to
			proto2:/server-b/dir-b/file-b
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\label{sec:Stork-Advanced}Running Stork Jobs Under DAGMan}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\index{Stork!as jobs with DAGMan}

Condor DAGMan, can provide high level management of both traditional CPU jobs,
and Stork data placement jobs.  See the DAGMan user manual, section
\ref{sec:DAGMan}, for details.  Using DAGMan, users can specify data placement
using the \Keyword{DATA} keyword.  Users can specify pure data transfer DAGs
comprised of only Stork jobs.  Or, mixed DAGs comprised of both Stork and
Condor jobs may be specified.  This capability lends itself well to grid
computing use cases, where jobs first stage in several input data sets.  After
all input data sets are ready, the CPU processing of the data can begin.
Afterwards, the processed data set can be staged out.

Here is a sample DAG that stages in 2 input files via Stork, processes the data
via a Condor job, and stage out the processed data set via Stork.

\begin{verbatim}
# Transfer input files using Stork
DATA INPUT1	transfer_input_data1.stork
DATA INPUT1	transfer_input_data2.stork

DATA INPUT2	transfer_data
#
# Process the data using Condor
JOB PROCESS process.condor
#
# Transfer output file using Stork
DATA OUTPUT	transfer_output_data1.stork
#
# Specify job dependencies
PARENT INPUT1 INPUT2 CHILD PROCESS
PARENT PROCESS CHILD OUTPUT
\end{verbatim}

